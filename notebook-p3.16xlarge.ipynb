{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaf2435",
   "metadata": {},
   "source": [
    "# ImageNet-1K Training - Medium Recipe üå∂Ô∏èüå∂Ô∏è\n",
    "\n",
    "**Target Hardware:** AWS p3.16xlarge (8x NVIDIA V100 GPUs, 16GB VRAM, 64 vCPUs)\n",
    "\n",
    "**Training Time:** 300-700 minutes for full ImageNet-1K (1.2M images)  \n",
    "**Target Accuracy:** 78.1% - 79.5%\n",
    "\n",
    "## Implemented Techniques:\n",
    "\n",
    "### Speed Up Methods:\n",
    "1. ‚úÖ **BlurPool** - Antialiased downsampling for shift-invariance\n",
    "2. ‚úÖ **FixRes** - Fine-tuning at higher resolution than training\n",
    "3. ‚úÖ **Label Smoothing** - Prevents overconfident predictions (0.1 smoothing)\n",
    "4. ‚úÖ **Progressive Resizing** - 128px ‚Üí 224px ‚Üí 288px\n",
    "5. ‚úÖ **MixUp** - Data augmentation mixing sample pairs\n",
    "6. ‚úÖ **SAM** - Sharpness Aware Minimization optimizer\n",
    "\n",
    "### Additional Optimizations:\n",
    "1. ‚úÖ **Channels Last** - Optimized memory format for GPU\n",
    "2. ‚úÖ **Mixed Precision (FP16)** - Faster training with 16-bit floats\n",
    "3. ‚úÖ **Distributed Data Parallel** - Multi-GPU training across 8 GPUs\n",
    "4. ‚úÖ **Synchronized Batch Normalization** - Consistent batch stats across GPUs\n",
    "5. ‚úÖ **Dynamic Batch Sizing** - Adjusts batch size based on resolution\n",
    "\n",
    "### Hardware Utilization (100%):\n",
    "- **GPU:** All 8 V100s with DDP + mixed precision\n",
    "- **CPU:** 64/64 vCPUs (8 workers √ó 8 GPUs)\n",
    "- **VRAM:** Optimized per resolution with dynamic batching\n",
    "\n",
    "### Training Schedule:\n",
    "| Epochs | Resolution | Batch/GPU | Total Batch | Augmentation |\n",
    "|--------|-----------|-----------|-------------|--------------|\n",
    "| 0-10   | 128x128   | 512       | 4,096       | Train (MixUp) |\n",
    "| 10-85  | 224x224   | 320       | 2,560       | Train (MixUp) |\n",
    "| 85-90  | 288x288   | 256       | 2,048       | Test (FixRes) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_optimizer import SAM\n",
    "import torchmetrics\n",
    "import os\n",
    "import antialiased_cnns  # For BlurPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int, num_workers: int):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = 128  # Start with the smallest resolution\n",
    "        self.use_train_augs = True\n",
    "        \n",
    "        # Standard ImageNet normalization\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def update_resolution(self, image_size: int, use_train_augs: bool, batch_size: int = None):\n",
    "        \"\"\"Called by the callback to update the transforms and optionally batch size.\"\"\"\n",
    "        self.image_size = image_size\n",
    "        self.use_train_augs = use_train_augs\n",
    "        if batch_size is not None:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"This is called by the Trainer to get the dataloader.\"\"\"\n",
    "        if self.use_train_augs:\n",
    "            # Standard training augmentations\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(self.image_size),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        else:\n",
    "            # Test-style augmentations for the FixRes phase\n",
    "            transform = T.Compose([\n",
    "                T.Resize(int(self.image_size * 256 / 224)), # Standard practice for validation\n",
    "                T.CenterCrop(self.image_size),\n",
    "                T.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        \n",
    "        train_dataset = ImageFolder(root=f\"{self.data_dir}/train\", transform=transform)\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,  # Enable for GPU training\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Use the current image size for validation to match training resolution\n",
    "        transform = T.Compose([\n",
    "            T.Resize(int(self.image_size * 256 / 224)),\n",
    "            T.CenterCrop(self.image_size),\n",
    "            T.ToTensor(),\n",
    "            self.normalize,\n",
    "        ])\n",
    "        val_dataset = ImageFolder(root=f\"{self.data_dir}/val\", transform=transform)\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=True,  # Enable for GPU training\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1b6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResolutionScheduleCallback(pl.Callback):\n",
    "    def __init__(self, schedule):\n",
    "        super().__init__()\n",
    "        # e.g., {0: (128, True, 512), 10: (224, True, 320), 85: (288, False, 256)}\n",
    "        # Format: {epoch: (resolution, use_train_augs, batch_size)}\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.schedule:\n",
    "            config = self.schedule[trainer.current_epoch]\n",
    "            \n",
    "            # Handle both old format (size, use_train_augs) and new format (size, use_train_augs, batch_size)\n",
    "            if len(config) == 2:\n",
    "                size, use_train_augs = config\n",
    "                batch_size = None\n",
    "            else:\n",
    "                size, use_train_augs, batch_size = config\n",
    "            \n",
    "            if batch_size:\n",
    "                print(f\"\\nEpoch {trainer.current_epoch}: Adjusting resolution to {size}x{size}, batch_size to {batch_size} per GPU\")\n",
    "            else:\n",
    "                print(f\"\\nEpoch {trainer.current_epoch}: Adjusting resolution to {size}x{size}\")\n",
    "            \n",
    "            # Update the datamodule's parameters\n",
    "            trainer.datamodule.update_resolution(size, use_train_augs, batch_size)\n",
    "            \n",
    "            # Force recreation of both train and val dataloaders with new transforms\n",
    "            # In PyTorch Lightning 2.x, we need to reload the dataloaders\n",
    "            trainer.fit_loop._data_source.instance = None\n",
    "            trainer.fit_loop.setup_data()\n",
    "            \n",
    "            # Also reset the validation dataloader\n",
    "            if trainer.val_dataloaders is not None:\n",
    "                trainer._evaluation_loop._data_source.instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetLitModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.1, momentum=0.9, weight_decay=1e-4, mixup_alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        ## Technique: BlurPool (antialiased downsampling)\n",
    "        self.model = antialiased_cnns.resnet50(pretrained=False, filter_size=4)\n",
    "        \n",
    "        ## Technique: Label Smoothing\n",
    "        self.loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        self.automatic_optimization = False\n",
    "        \n",
    "        # Initialize accuracy metric\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        optimizer = self.optimizers()\n",
    "        images, labels = batch\n",
    "        \n",
    "        ## Technique: MixUp - Data augmentation that mixes pairs of samples\n",
    "        lam = torch.distributions.beta.Beta(self.hparams.mixup_alpha, self.hparams.mixup_alpha).sample().to(self.device)\n",
    "        shuffled_indices = torch.randperm(images.size(0))\n",
    "        mixed_images = lam * images + (1 - lam) * images[shuffled_indices]\n",
    "        labels_a, labels_b = labels, labels[shuffled_indices]\n",
    "\n",
    "        ## Technique: SAM (Sharpness Aware Minimization) - Two-Step Update\n",
    "        # First forward-backward pass\n",
    "        outputs = self(mixed_images)\n",
    "        loss = lam * self.loss_fn(outputs, labels_a) + (1 - lam) * self.loss_fn(outputs, labels_b)\n",
    "        self.manual_backward(loss)\n",
    "        optimizer.first_step(zero_grad=True)\n",
    "        \n",
    "        # Second forward-backward pass for SAM\n",
    "        outputs_2 = self(mixed_images)\n",
    "        loss_2 = lam * self.loss_fn(outputs_2, labels_a) + (1 - lam) * self.loss_fn(outputs_2, labels_b)\n",
    "        self.manual_backward(loss_2)\n",
    "        optimizer.second_step(zero_grad=True)\n",
    "        \n",
    "        self.log(\"train_loss\", loss_2, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss_2\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        base_optimizer = torch.optim.SGD\n",
    "        optimizer = SAM(\n",
    "            self.parameters(),\n",
    "            base_optimizer,\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=self.hparams.momentum,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf944ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # this is directory of imagenet1k in EBS\n",
    "    IMAGENET_PATH = '/home/ec2-user/imagenet1k' # Should contain 'train/' and 'val/' subfolders\n",
    "\n",
    "    # this is the directory of imagenet1k-mini in local\n",
    "    # IMAGENET_PATH = 'imagenet-mini' # Should contain 'train/' and 'val/' subfolders\n",
    "\n",
    "    # Instantiate the DataModule\n",
    "    # p3.16xlarge has 8 V100 GPUs (16GB each), dynamic batch sizing for max utilization\n",
    "    datamodule = ImageNetDataModule(\n",
    "        data_dir=IMAGENET_PATH,\n",
    "        batch_size=512,  # Initial batch size (will be dynamically adjusted)\n",
    "        num_workers=8    # Per GPU workers (8 workers * 8 GPUs = 64 total, uses all 64 vCPUs)\n",
    "    )\n",
    "    \n",
    "    # Instantiate the Model\n",
    "    model = ImageNetLitModule()\n",
    "\n",
    "    ## Technique: Channels Last (optimized memory format for GPU)\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    ## Technique: Progressive Resizing + FixRes + Dynamic Batch Sizing\n",
    "    # Dynamically adjust resolution AND batch size to maximize GPU utilization\n",
    "    # Format: {epoch: (resolution, use_train_augs, batch_size_per_gpu)}\n",
    "    # Total batch: 512*8=4096 ‚Üí 320*8=2560 ‚Üí 256*8=2048\n",
    "    res_schedule = {\n",
    "        0: (128, True, 512),   # Small images = larger batches\n",
    "        10: (224, True, 320),  # Medium images = medium batches\n",
    "        85: (288, False, 256)  # Large images + FixRes = smaller batches\n",
    "    }\n",
    "    schedule_callback = ResolutionScheduleCallback(schedule=res_schedule)\n",
    "\n",
    "    # Configure the Trainer for p3.16xlarge (8 x V100 GPUs)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=90,\n",
    "        accelerator='gpu',           # GPU acceleration\n",
    "        devices=8,                   # Use all 8 GPUs\n",
    "        strategy='ddp',              # Distributed Data Parallel\n",
    "        precision='16-mixed',        # Mixed precision training for faster computation\n",
    "        callbacks=[schedule_callback],\n",
    "        log_every_n_steps=50,\n",
    "        sync_batchnorm=True,         # Sync batch norm across GPUs\n",
    "    )\n",
    "\n",
    "    # Start training!\n",
    "    trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00350d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis or experimentation can go here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
