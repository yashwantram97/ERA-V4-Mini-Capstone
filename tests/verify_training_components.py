"""
Verification Script for Training Components

This script verifies the following before running on GPU/AWS:
1. Image augmentation is working properly
2. OneCycle policy is configured correctly
3. Resolution changing works as expected
4. BlurPool is properly integrated
5. FixRes is working correctly

Run this script before deploying to expensive GPU instances!
"""

import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from PIL import Image
import torchvision.transforms as T

# Import project modules
from src.utils.utils import get_transforms
from src.data_modules.imagenet_datamodule import ImageNetDataModule
from src.models.resnet_module import ResnetLightningModule
from src.callbacks.resolution_schedule_callback import ResolutionScheduleCallback
from configs.local_config import *
import lightning as L
from torch.utils.data import DataLoader

print("="*80)
print("üîç TRAINING COMPONENTS VERIFICATION")
print("="*80)

# ============================================================================
# TEST 1: Image Augmentation Verification
# ============================================================================
def test_augmentations():
    """Verify that image augmentations are applied correctly"""
    print("\n" + "="*80)
    print("TEST 1: IMAGE AUGMENTATION VERIFICATION")
    print("="*80)
    
    # Get a sample image from the dataset
    sample_img_path = list(TRAIN_IMG_DIR.glob("*//*.JPEG"))[0]
    print(f"\nüì∑ Loading sample image: {sample_img_path.name}")
    
    # Load image (keep as PIL for torchvision)
    image = Image.open(sample_img_path).convert('RGB')
    
    print(f"   Original size: {image.size}")
    
    # Test train transforms
    print("\nüîß Testing TRAIN transforms (with augmentation):")
    train_transforms = get_transforms(
        transform_type="train",
        mean=MEAN,
        std=STD,
        resolution=224
    )
    
    # Print transform pipeline
    for i, transform in enumerate(train_transforms.transforms, 1):
        print(f"   {i}. {transform.__class__.__name__}")
    
    # Apply transforms multiple times to see variation
    print("\n   Applying transforms 5 times to verify randomness...")
    transformed_images = []
    for i in range(5):
        transformed = train_transforms(image)
        transformed_images.append(transformed)
        # Check shape after transform
        if i == 0:
            print(f"   ‚úÖ Output shape: {transformed.shape}")
            print(f"   ‚úÖ Output type: {type(transformed)}")
    
    # Verify randomness (images should be different)
    all_same = all(torch.equal(transformed_images[0], img) for img in transformed_images[1:])
    if all_same:
        print("   ‚ùå WARNING: All transformed images are identical!")
        print("      Random augmentations may not be working properly.")
    else:
        print("   ‚úÖ Random augmentations are working (images differ)")
    
    # Test validation transforms
    print("\nüîß Testing VALIDATION transforms (FixRes style):")
    val_transforms = get_transforms(
        transform_type="valid",
        mean=MEAN,
        std=STD,
        resolution=224
    )
    
    for i, transform in enumerate(val_transforms.transforms, 1):
        print(f"   {i}. {transform.__class__.__name__}")
    
    val_transformed = val_transforms(image)
    print(f"   ‚úÖ Output shape: {val_transformed.shape}")
    
    # Test FixRes transforms (higher resolution)
    print("\nüîß Testing FIXRES transforms (288px with test-time augmentations):")
    fixres_transforms = get_transforms(
        transform_type="valid",  # Use validation transforms for FixRes
        mean=MEAN,
        std=STD,
        resolution=288
    )
    
    for i, transform in enumerate(fixres_transforms.transforms, 1):
        print(f"   {i}. {transform.__class__.__name__}")
    
    fixres_transformed = fixres_transforms(image)
    print(f"   ‚úÖ Output shape: {fixres_transformed.shape}")
    
    print("\n‚úÖ TEST 1 PASSED: Augmentations are working correctly")
    return True

# ============================================================================
# TEST 2: OneCycle Policy Verification
# ============================================================================
def test_onecycle_policy():
    """Verify OneCycle LR policy configuration"""
    print("\n" + "="*80)
    print("TEST 2: ONECYCLE POLICY VERIFICATION")
    print("="*80)
    
    # Create a dummy model and optimizer
    model = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES
    )
    
    # We need to simulate the trainer context
    # Create a minimal trainer to test the scheduler
    datamodule = ImageNetDataModule(
        train_img_dir=str(TRAIN_IMG_DIR),
        val_img_dir=str(VAL_IMG_DIR),
        mean=MEAN,
        std=STD,
        batch_size=BATCH_SIZE,
        num_workers=0,  # Use 0 workers for quick test
        initial_resolution=224
    )
    
    # Setup datamodule to get the dataloaders
    datamodule.setup('fit')
    
    # Calculate steps
    train_loader = datamodule.train_dataloader()
    steps_per_epoch = len(train_loader)
    total_steps = steps_per_epoch * EPOCHS
    
    print(f"\nüìä Training Configuration:")
    print(f"   Epochs: {EPOCHS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Train samples: {len(datamodule.train_dataset)}")
    print(f"   Steps per epoch: {steps_per_epoch}")
    print(f"   Total steps: {total_steps}")
    
    # Create optimizer
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        momentum=0.9
    )
    
    # Create OneCycle scheduler
    scheduler = torch.optim.lr_scheduler.OneCycleLR(
        optimizer,
        max_lr=LEARNING_RATE,
        epochs=EPOCHS,
        steps_per_epoch=steps_per_epoch,
        pct_start=0.2,
        anneal_strategy='cos',
        cycle_momentum=True,
        base_momentum=0.85,
        max_momentum=0.95,
        div_factor=100.0,
        final_div_factor=1000.0
    )
    
    print(f"\nüîÑ OneCycle Scheduler Configuration:")
    print(f"   Max LR: {LEARNING_RATE:.4e}")
    print(f"   Initial LR: {LEARNING_RATE/100.0:.4e}")
    print(f"   Final LR: {LEARNING_RATE/100.0/1000.0:.4e}")
    print(f"   Warmup steps: {int(total_steps * 0.2)}")
    print(f"   Annealing steps: {int(total_steps * 0.8)}")
    
    # Simulate training and collect LR values
    print("\nüìà Simulating LR schedule over epochs...")
    lr_history = []
    momentum_history = []
    
    for epoch in range(EPOCHS):
        for step in range(steps_per_epoch):
            lr_history.append(optimizer.param_groups[0]['lr'])
            momentum_history.append(optimizer.param_groups[0]['momentum'])
            scheduler.step()
    
    # Verify LR schedule characteristics
    print(f"\n‚úÖ LR Schedule Analysis:")
    print(f"   Starting LR: {lr_history[0]:.4e}")
    print(f"   Maximum LR: {max(lr_history):.4e}")
    print(f"   Final LR: {lr_history[-1]:.4e}")
    print(f"   LR at 20% (end of warmup): {lr_history[int(len(lr_history)*0.2)]:.4e}")
    
    # Check if LR follows expected pattern
    warmup_end_idx = int(len(lr_history) * 0.2)
    is_warmup_increasing = lr_history[warmup_end_idx] > lr_history[0]
    is_annealing_decreasing = lr_history[-1] < max(lr_history)
    
    if is_warmup_increasing and is_annealing_decreasing:
        print("\n   ‚úÖ LR schedule follows correct pattern:")
        print("      - Warmup phase: LR increases")
        print("      - Annealing phase: LR decreases")
    else:
        print("\n   ‚ùå WARNING: LR schedule may not be correct!")
    
    # Check momentum schedule
    print(f"\n‚úÖ Momentum Schedule Analysis:")
    print(f"   Starting momentum: {momentum_history[0]:.4f}")
    print(f"   Minimum momentum: {min(momentum_history):.4f}")
    print(f"   Final momentum: {momentum_history[-1]:.4f}")
    
    print("\n‚úÖ TEST 2 PASSED: OneCycle policy is configured correctly")
    return True, lr_history, momentum_history

# ============================================================================
# TEST 3: Resolution Schedule Verification
# ============================================================================
def test_resolution_schedule():
    """Verify resolution scheduling works correctly"""
    print("\n" + "="*80)
    print("TEST 3: RESOLUTION SCHEDULE VERIFICATION")
    print("="*80)
    
    # Print the configured schedule
    print(f"\nüìê Configured Resolution Schedule:")
    for epoch, (resolution, use_train_augs) in sorted(PROG_RESIZING_FIXRES_SCHEDULE.items()):
        aug_type = "Train" if use_train_augs else "Test (FixRes)"
        print(f"   Epoch {epoch:2d}+: {resolution}x{resolution}px, {aug_type} augmentations")
    
    # Create callback
    callback = ResolutionScheduleCallback(PROG_RESIZING_FIXRES_SCHEDULE)
    
    # Verify callback initialization
    print(f"\n‚úÖ Callback created successfully")
    print(f"   Schedule epochs: {sorted(callback.schedule.keys())}")
    
    # Test resolution changes manually
    print(f"\nüîß Testing resolution changes:")
    
    for epoch, (resolution, use_train_augs) in sorted(PROG_RESIZING_FIXRES_SCHEDULE.items()):
        print(f"\n   Epoch {epoch}:")
        
        # Get transforms for this configuration
        train_transforms = get_transforms(
            transform_type="train" if use_train_augs else "valid",
            mean=MEAN,
            std=STD,
            resolution=resolution
        )
        
        val_transforms = get_transforms(
            transform_type="valid",
            mean=MEAN,
            std=STD,
            resolution=resolution
        )
        
        # Check transform output sizes - create PIL image
        from PIL import Image
        sample_img = Image.fromarray(np.random.randint(0, 255, (500, 500, 3), dtype=np.uint8))
        
        train_result = train_transforms(sample_img)
        val_result = val_transforms(sample_img)
        
        print(f"      Train output: {train_result.shape}")
        print(f"      Val output: {val_result.shape}")
        
        # Verify resolution matches expected
        expected_shape = (3, resolution, resolution)
        if train_result.shape == expected_shape:
            print(f"      ‚úÖ Train transform produces correct size")
        else:
            print(f"      ‚ùå Train transform size mismatch: expected {expected_shape}, got {train_result.shape}")
        
        if val_result.shape == expected_shape:
            print(f"      ‚úÖ Val transform produces correct size")
        else:
            print(f"      ‚ùå Val transform size mismatch: expected {expected_shape}, got {val_result.shape}")
    
    print("\n‚úÖ TEST 3 PASSED: Resolution scheduling is working correctly")
    return True

# ============================================================================
# TEST 4: BlurPool Verification
# ============================================================================
def test_blurpool():
    """Verify BlurPool is integrated in the model"""
    print("\n" + "="*80)
    print("TEST 4: BLURPOOL VERIFICATION")
    print("="*80)
    
    # Create model
    model = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES
    )
    
    print(f"\nüîç Checking model architecture for BlurPool...")
    
    # Check if antialiased_cnns is being used
    import antialiased_cnns
    print(f"   ‚úÖ antialiased_cnns imported successfully")
    print(f"   Model type: {type(model.model).__name__}")
    
    # Check for blur pool layers in the model
    blurpool_count = 0
    maxpool_count = 0
    avgpool_count = 0
    
    for name, module in model.model.named_modules():
        module_name = module.__class__.__name__
        if 'blur' in module_name.lower() or 'BlurPool' in module_name:
            blurpool_count += 1
            print(f"   ‚úÖ Found BlurPool layer: {name} ({module_name})")
        elif module_name == 'MaxPool2d':
            maxpool_count += 1
        elif module_name == 'AvgPool2d':
            avgpool_count += 1
    
    print(f"\nüìä Pooling Layer Summary:")
    print(f"   BlurPool layers: {blurpool_count}")
    print(f"   MaxPool layers: {maxpool_count}")
    print(f"   AvgPool layers: {avgpool_count}")
    
    if blurpool_count > 0:
        print(f"\n   ‚úÖ BlurPool is integrated (found {blurpool_count} layers)")
    else:
        print(f"\n   ‚ö†Ô∏è  No BlurPool layers found explicitly named")
        print(f"      This is OK if antialiased_cnns replaces MaxPool internally")
    
    # Test forward pass with BlurPool
    print(f"\nüß™ Testing forward pass with BlurPool model...")
    model.eval()
    with torch.no_grad():
        sample_input = torch.randn(2, 3, 224, 224)
        output = model(sample_input)
        print(f"   Input shape: {sample_input.shape}")
        print(f"   Output shape: {output.shape}")
        print(f"   ‚úÖ Forward pass successful")
    
    print("\n‚úÖ TEST 4 PASSED: BlurPool is properly integrated")
    return True

# ============================================================================
# TEST 5: FixRes Verification
# ============================================================================
def test_fixres():
    """Verify FixRes (test-time augmentations at higher resolution)"""
    print("\n" + "="*80)
    print("TEST 5: FIXRES VERIFICATION")
    print("="*80)
    
    print(f"\nüìö FixRes Concept:")
    print(f"   - Train with aggressive augmentations (RandomResizedCrop, flips)")
    print(f"   - Fine-tune final epochs with test-time augmentations (Resize + CenterCrop)")
    print(f"   - Use higher resolution in final phase for better accuracy")
    
    # Check FixRes configuration from schedule
    fixres_epochs = [(epoch, res, aug) for epoch, (res, aug) in PROG_RESIZING_FIXRES_SCHEDULE.items() if not aug]
    
    if not fixres_epochs:
        print(f"\n   ‚ùå WARNING: No FixRes phase found in schedule!")
        print(f"      (Looking for epochs with use_train_augs=False)")
        return False
    
    print(f"\n‚úÖ FixRes Configuration Found:")
    for epoch, resolution, _ in fixres_epochs:
        print(f"   Epoch {epoch}+: {resolution}x{resolution}px with test-time augmentations")
    
    # Test the transforms for FixRes phase
    fixres_epoch, fixres_resolution, _ = fixres_epochs[0]
    
    print(f"\nüîß Testing FixRes transforms (Epoch {fixres_epoch}, {fixres_resolution}px):")
    
    # Train transforms (before FixRes)
    train_transforms = get_transforms(
        transform_type="train",
        mean=MEAN,
        std=STD,
        resolution=fixres_resolution
    )
    
    print(f"\n   Train augmentations (used in early epochs):")
    for i, t in enumerate(train_transforms.transforms, 1):
        print(f"      {i}. {t.__class__.__name__}")
    
    # FixRes transforms (test-time)
    fixres_transforms = get_transforms(
        transform_type="valid",  # Note: using 'valid' for FixRes
        mean=MEAN,
        std=STD,
        resolution=fixres_resolution
    )
    
    print(f"\n   FixRes augmentations (used in final epochs):")
    for i, t in enumerate(fixres_transforms.transforms, 1):
        print(f"      {i}. {t.__class__.__name__}")
    
    # Verify the key differences
    train_has_random = any('Random' in t.__class__.__name__ for t in train_transforms.transforms)
    fixres_has_random = any('Random' in t.__class__.__name__ for t in fixres_transforms.transforms)
    fixres_has_center_crop = any('CenterCrop' in t.__class__.__name__ for t in fixres_transforms.transforms)
    
    print(f"\n‚úÖ FixRes Verification:")
    print(f"   Train has random augmentations: {train_has_random}")
    print(f"   FixRes has random augmentations: {fixres_has_random}")
    print(f"   FixRes has CenterCrop: {fixres_has_center_crop}")
    
    if train_has_random and not fixres_has_random and fixres_has_center_crop:
        print(f"\n   ‚úÖ FixRes is correctly configured:")
        print(f"      - Train phase uses random augmentations")
        print(f"      - FixRes phase uses deterministic test-time augmentations")
    else:
        print(f"\n   ‚ö†Ô∏è  FixRes configuration may need review")
    
    # Test transforms on sample image
    print(f"\nüß™ Testing FixRes transforms on sample image:")
    sample_img = Image.fromarray(np.random.randint(0, 255, (600, 800, 3), dtype=np.uint8))
    
    # Apply train transforms multiple times
    train_results = [train_transforms(sample_img) for _ in range(3)]
    
    # Apply FixRes transforms multiple times
    fixres_results = [fixres_transforms(sample_img) for _ in range(3)]
    
    # Check if train transforms are random
    train_all_same = all(torch.equal(train_results[0], img) for img in train_results[1:])
    print(f"   Train transforms are deterministic: {train_all_same}")
    
    # Check if FixRes transforms are deterministic
    fixres_all_same = all(torch.equal(fixres_results[0], img) for img in fixres_results[1:])
    print(f"   FixRes transforms are deterministic: {fixres_all_same}")
    
    if not train_all_same and fixres_all_same:
        print(f"\n   ‚úÖ Perfect! Train varies, FixRes is consistent")
    
    print("\n‚úÖ TEST 5 PASSED: FixRes is working correctly")
    return True

# ============================================================================
# TEST 6: MixUp Verification
# ============================================================================
def test_mixup():
    """Verify MixUp augmentation is working correctly"""
    print("\n" + "="*80)
    print("TEST 6: MIXUP VERIFICATION")
    print("="*80)
    
    print(f"\nüìö MixUp Concept:")
    print(f"   - MixUp mixes pairs of training samples and their labels")
    print(f"   - Creates synthetic training examples by linear interpolation")
    print(f"   - Helps with regularization and generalization")
    print(f"   - Formula: mixed_image = Œª * image_a + (1-Œª) * image_b")
    print(f"   - Œª is sampled from Beta(alpha, alpha) distribution")
    
    # Test 1: MixUp initialization with enabled config
    print(f"\nüîß TEST 6.1: MixUp Initialization (Enabled)")
    print("="*60)
    
    mixup_kwargs_enabled = {
        'mixup_alpha': 0.2,
        'cutmix_alpha': 0.0,
        'prob': 1.0,
        'mode': 'batch',
        'label_smoothing': 0.1,
    }
    
    model_with_mixup = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=mixup_kwargs_enabled
    )
    
    if model_with_mixup.mixup_cutmix_fn is not None:
        print(f"   ‚úÖ MixUp enabled successfully")
        print(f"   MixUp alpha: {mixup_kwargs_enabled['mixup_alpha']}")
        print(f"   Probability: {mixup_kwargs_enabled['prob']}")
        print(f"   Mode: {mixup_kwargs_enabled['mode']}")
    else:
        print(f"   ‚ùå MixUp initialization failed!")
        return False
    
    # Test 2: MixUp initialization with disabled config
    print(f"\nüîß TEST 6.2: MixUp Initialization (Disabled)")
    print("="*60)
    
    mixup_kwargs_disabled = {
        'mixup_alpha': 0.0,  # Disabled
    }
    
    model_without_mixup = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=mixup_kwargs_disabled
    )
    
    if model_without_mixup.mixup_cutmix_fn is None:
        print(f"   ‚úÖ MixUp correctly disabled when alpha=0")
    else:
        print(f"   ‚ùå MixUp should be disabled when alpha=0!")
    
    # Test 3: MixUp with None kwargs
    print(f"\nüîß TEST 6.3: MixUp with None kwargs")
    print("="*60)
    
    model_no_kwargs = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=None
    )
    
    if model_no_kwargs.mixup_cutmix_fn is None:
        print(f"   ‚úÖ MixUp correctly disabled when kwargs=None")
    else:
        print(f"   ‚ùå MixUp should be disabled when kwargs=None!")
    
    # Test 4: MixUp transformation on actual data
    print(f"\nüîß TEST 6.4: MixUp Transformation on Data")
    print("="*60)
    
    # Create sample batch
    batch_size = 8
    sample_images = torch.randn(batch_size, 3, 224, 224)
    sample_labels = torch.randint(0, NUM_CLASSES, (batch_size,))
    
    print(f"\n   Original batch:")
    print(f"      Image shape: {sample_images.shape}")
    print(f"      Label shape: {sample_labels.shape}")
    print(f"      Label type: {sample_labels.dtype}")
    print(f"      Sample labels: {sample_labels[:4].tolist()}")
    
    # Apply MixUp
    mixed_images, mixed_labels = model_with_mixup.mixup_cutmix_fn(sample_images, sample_labels)
    
    print(f"\n   After MixUp:")
    print(f"      Image shape: {mixed_images.shape}")
    print(f"      Label shape: {mixed_labels.shape}")
    print(f"      Label type: {mixed_labels.dtype}")
    
    # Verify shapes
    if mixed_images.shape == sample_images.shape:
        print(f"   ‚úÖ Mixed images have correct shape")
    else:
        print(f"   ‚ùå Mixed images shape mismatch!")
        return False
    
    # Verify labels are now soft (one-hot encoded)
    if mixed_labels.shape == (batch_size, NUM_CLASSES):
        print(f"   ‚úÖ Labels converted to soft labels (one-hot)")
    else:
        print(f"   ‚ùå Labels shape incorrect: expected ({batch_size}, {NUM_CLASSES}), got {mixed_labels.shape}")
        return False
    
    # Verify labels sum to 1 (probability distribution)
    label_sums = mixed_labels.sum(dim=1)
    if torch.allclose(label_sums, torch.ones_like(label_sums), atol=1e-5):
        print(f"   ‚úÖ Soft labels sum to 1.0 (valid probability distribution)")
    else:
        print(f"   ‚ùå Soft labels don't sum to 1.0!")
        print(f"      Sample sums: {label_sums[:4].tolist()}")
        return False
    
    # Check that labels are actually mixed (not just one-hot)
    # Count how many labels have non-zero values in more than one class
    non_zero_per_sample = (mixed_labels > 0.01).sum(dim=1)
    mixed_samples = (non_zero_per_sample > 1).sum().item()
    
    print(f"\n   üìä MixUp Statistics:")
    print(f"      Samples with mixed labels: {mixed_samples}/{batch_size}")
    print(f"      Average classes per sample: {non_zero_per_sample.float().mean():.2f}")
    
    if mixed_samples > 0:
        print(f"   ‚úÖ Labels are being mixed (not just one-hot)")
        # Show example of a mixed label
        mixed_idx = (non_zero_per_sample > 1).nonzero()[0].item()
        non_zero_classes = (mixed_labels[mixed_idx] > 0.01).nonzero().squeeze()
        print(f"\n   Example mixed label (sample {mixed_idx}):")
        for cls_idx in non_zero_classes[:3]:  # Show up to 3 classes
            cls_idx = cls_idx.item() if cls_idx.numel() > 0 else cls_idx
            print(f"      Class {cls_idx}: {mixed_labels[mixed_idx, cls_idx]:.3f}")
    else:
        print(f"   ‚ö†Ô∏è  No labels were mixed in this batch (could be random)")
    
    # Test 5: MixUp affects images
    print(f"\nüîß TEST 6.5: MixUp Image Mixing Verification")
    print("="*60)
    
    # Create two distinct images
    image_a = torch.ones(1, 3, 224, 224) * 0.0  # Black image
    image_b = torch.ones(1, 3, 224, 224) * 1.0  # White image
    
    # Stack them
    test_images = torch.cat([image_a, image_b], dim=0)
    test_labels = torch.tensor([0, 1])
    
    # Apply MixUp multiple times and check for variation
    print(f"   Testing MixUp randomness (10 iterations)...")
    mixed_results = []
    lambda_values = []
    
    for i in range(10):
        mixed_imgs, mixed_lbls = model_with_mixup.mixup_cutmix_fn(test_images.clone(), test_labels.clone())
        # Get the mean pixel value of first image (should be between 0 and 1)
        mean_val = mixed_imgs[0].mean().item()
        mixed_results.append(mean_val)
        # Estimate lambda from the mixed label
        lambda_val = mixed_lbls[0].max().item()
        lambda_values.append(lambda_val)
    
    # Check if we got different results
    unique_results = len(set([round(x, 3) for x in mixed_results]))
    
    print(f"\n   üìä Randomness check:")
    print(f"      Unique mean values: {unique_results}/10")
    print(f"      Mean values range: [{min(mixed_results):.3f}, {max(mixed_results):.3f}]")
    print(f"      Lambda range: [{min(lambda_values):.3f}, {max(lambda_values):.3f}]")
    
    if unique_results > 1:
        print(f"   ‚úÖ MixUp produces different results (randomness working)")
    else:
        print(f"   ‚ö†Ô∏è  MixUp produces same results (may not be random)")
    
    # Test 6: Integration with training step
    print(f"\nüîß TEST 6.6: MixUp Integration in Training Step")
    print("="*60)
    
    # Create a sample batch
    batch_images = torch.randn(4, 3, 224, 224)
    batch_labels = torch.randint(0, NUM_CLASSES, (4,))
    
    # Test with MixUp enabled
    model_with_mixup.eval()  # Set to eval to avoid any other randomness
    with torch.no_grad():
        try:
            # Simulate training step logic
            if model_with_mixup.mixup_cutmix_fn is not None:
                mixed_images, mixed_labels = model_with_mixup.mixup_cutmix_fn(batch_images, batch_labels)
                logits = model_with_mixup(mixed_images)
                loss = F.cross_entropy(logits, mixed_labels)
                print(f"   ‚úÖ Training step with MixUp successful")
                print(f"      Loss: {loss.item():.4f}")
                print(f"      Logits shape: {logits.shape}")
            else:
                print(f"   ‚ùå MixUp should be enabled!")
                return False
        except Exception as e:
            print(f"   ‚ùå Error in training step with MixUp: {e}")
            return False
    
    # Test without MixUp
    model_without_mixup.eval()
    with torch.no_grad():
        try:
            if model_without_mixup.mixup_cutmix_fn is None:
                logits = model_without_mixup(batch_images)
                loss = F.cross_entropy(logits, batch_labels, label_smoothing=0.1)
                print(f"   ‚úÖ Training step without MixUp successful")
                print(f"      Loss: {loss.item():.4f}")
            else:
                print(f"   ‚ùå MixUp should be disabled!")
                return False
        except Exception as e:
            print(f"   ‚ùå Error in training step without MixUp: {e}")
            return False
    
    # Test 7: Check configuration from config file
    print(f"\nüîß TEST 6.7: MixUp Configuration from Config")
    print("="*60)
    
    print(f"\n   Current MixUp configuration:")
    for key, value in MIXUP_KWARGS.items():
        print(f"      {key}: {value}")
    
    # Verify reasonable values
    checks = []
    
    if 0.0 <= MIXUP_KWARGS.get('mixup_alpha', 0.0) <= 1.0:
        print(f"   ‚úÖ mixup_alpha in valid range [0.0, 1.0]")
        checks.append(True)
    else:
        print(f"   ‚ùå mixup_alpha out of range!")
        checks.append(False)
    
    if 0.0 <= MIXUP_KWARGS.get('prob', 1.0) <= 1.0:
        print(f"   ‚úÖ prob in valid range [0.0, 1.0]")
        checks.append(True)
    else:
        print(f"   ‚ùå prob out of range!")
        checks.append(False)
    
    if MIXUP_KWARGS.get('mode', 'batch') in ['batch', 'pair', 'elem']:
        print(f"   ‚úÖ mode is valid")
        checks.append(True)
    else:
        print(f"   ‚ùå mode is invalid!")
        checks.append(False)
    
    if not all(checks):
        return False
    
    print("\n‚úÖ TEST 6 PASSED: MixUp is working correctly")
    return True

# ============================================================================
# TEST 7: CutMix Verification
# ============================================================================
def test_cutmix():
    """Verify CutMix augmentation is working correctly"""
    print("\n" + "="*80)
    print("TEST 7: CUTMIX VERIFICATION")
    print("="*80)
    
    print(f"\nüìö CutMix Concept:")
    print(f"   - CutMix cuts and pastes patches between training images")
    print(f"   - Creates synthetic training examples by replacing rectangular regions")
    print(f"   - Helps with regularization and localization")
    print(f"   - Formula: cut region from image_b and paste into image_a")
    print(f"   - Œª (mixing ratio) is based on the area of the cut region")
    
    # Test 1: CutMix initialization with enabled config
    print(f"\nüîß TEST 7.1: CutMix Initialization (Enabled)")
    print("="*60)
    
    cutmix_kwargs_enabled = {
        'mixup_alpha': 0.0,      # Disable MixUp
        'cutmix_alpha': 1.0,     # Enable CutMix
        'prob': 1.0,
        'mode': 'batch',
        'label_smoothing': 0.1,
    }
    
    model_with_cutmix = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=cutmix_kwargs_enabled
    )
    
    if model_with_cutmix.mixup_cutmix_fn is not None:
        print(f"   ‚úÖ CutMix enabled successfully")
        print(f"   CutMix alpha: {cutmix_kwargs_enabled['cutmix_alpha']}")
        print(f"   Probability: {cutmix_kwargs_enabled['prob']}")
        print(f"   Mode: {cutmix_kwargs_enabled['mode']}")
    else:
        print(f"   ‚ùå CutMix initialization failed!")
        return False
    
    # Test 2: CutMix initialization with disabled config
    print(f"\nüîß TEST 7.2: CutMix Initialization (Disabled)")
    print("="*60)
    
    cutmix_kwargs_disabled = {
        'mixup_alpha': 0.0,
        'cutmix_alpha': 0.0,  # Disabled
    }
    
    model_without_cutmix = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=cutmix_kwargs_disabled
    )
    
    if model_without_cutmix.mixup_cutmix_fn is None:
        print(f"   ‚úÖ CutMix correctly disabled when both alphas=0")
    else:
        print(f"   ‚ùå CutMix should be disabled when both alphas=0!")
    
    # Test 3: CutMix transformation on actual data
    print(f"\nüîß TEST 7.3: CutMix Transformation on Data")
    print("="*60)
    
    # Create sample batch
    batch_size = 8
    sample_images = torch.randn(batch_size, 3, 224, 224)
    sample_labels = torch.randint(0, NUM_CLASSES, (batch_size,))
    
    print(f"\n   Original batch:")
    print(f"      Image shape: {sample_images.shape}")
    print(f"      Label shape: {sample_labels.shape}")
    print(f"      Sample labels: {sample_labels[:4].tolist()}")
    
    # Store original images for comparison
    original_images = sample_images.clone()
    
    # Apply CutMix
    cutmixed_images, cutmixed_labels = model_with_cutmix.mixup_cutmix_fn(sample_images, sample_labels)
    
    print(f"\n   After CutMix:")
    print(f"      Image shape: {cutmixed_images.shape}")
    print(f"      Label shape: {cutmixed_labels.shape}")
    
    # Verify shapes
    if cutmixed_images.shape == original_images.shape:
        print(f"   ‚úÖ CutMix images have correct shape")
    else:
        print(f"   ‚ùå CutMix images shape mismatch!")
        return False
    
    # Verify labels are now soft (one-hot encoded)
    if cutmixed_labels.shape == (batch_size, NUM_CLASSES):
        print(f"   ‚úÖ Labels converted to soft labels (one-hot)")
    else:
        print(f"   ‚ùå Labels shape incorrect: expected ({batch_size}, {NUM_CLASSES}), got {cutmixed_labels.shape}")
        return False
    
    # Verify labels sum to 1 (probability distribution)
    label_sums = cutmixed_labels.sum(dim=1)
    if torch.allclose(label_sums, torch.ones_like(label_sums), atol=1e-5):
        print(f"   ‚úÖ Soft labels sum to 1.0 (valid probability distribution)")
    else:
        print(f"   ‚ùå Soft labels don't sum to 1.0!")
        print(f"      Sample sums: {label_sums[:4].tolist()}")
        return False
    
    # Verify that images were modified (CutMix should change the images)
    images_changed = not torch.allclose(cutmixed_images, original_images)
    if images_changed:
        print(f"   ‚úÖ Images were modified by CutMix")
    else:
        print(f"   ‚ö†Ô∏è  Images weren't modified (could be random)")
    
    # Check that labels are actually mixed (not just one-hot)
    non_zero_per_sample = (cutmixed_labels > 0.01).sum(dim=1)
    mixed_samples = (non_zero_per_sample > 1).sum().item()
    
    print(f"\n   üìä CutMix Statistics:")
    print(f"      Samples with mixed labels: {mixed_samples}/{batch_size}")
    print(f"      Average classes per sample: {non_zero_per_sample.float().mean():.2f}")
    
    if mixed_samples > 0:
        print(f"   ‚úÖ Labels are being mixed (not just one-hot)")
        # Show example of a mixed label
        mixed_idx = (non_zero_per_sample > 1).nonzero()[0].item()
        non_zero_classes = (cutmixed_labels[mixed_idx] > 0.01).nonzero().squeeze()
        print(f"\n   Example mixed label (sample {mixed_idx}):")
        for cls_idx in non_zero_classes[:3]:  # Show up to 3 classes
            cls_idx = cls_idx.item() if cls_idx.numel() > 0 else cls_idx
            print(f"      Class {cls_idx}: {cutmixed_labels[mixed_idx, cls_idx]:.3f}")
    
    # Test 4: Verify CutMix behavior (rectangular region mixing)
    print(f"\nüîß TEST 7.4: CutMix Rectangular Region Verification")
    print("="*60)
    
    # Create two distinct images to see the effect
    image_a = torch.zeros(1, 3, 224, 224)  # Black image
    image_b = torch.ones(1, 3, 224, 224)   # White image
    
    # Stack them
    test_images = torch.cat([image_a, image_b], dim=0)
    test_labels = torch.tensor([0, 1])
    
    # Apply CutMix multiple times and check for rectangular patterns
    print(f"   Testing CutMix pattern (10 iterations)...")
    cutmix_ratios = []
    
    for i in range(10):
        cut_imgs, cut_lbls = model_with_cutmix.mixup_cutmix_fn(test_images.clone(), test_labels.clone())
        
        # Check the first image - should have rectangular region from second image
        # Count white pixels (value close to 1.0)
        white_pixels = (cut_imgs[0] > 0.9).float().sum()
        total_pixels = cut_imgs[0].numel()
        white_ratio = white_pixels / total_pixels
        
        cutmix_ratios.append(white_ratio.item())
        
        # Get lambda from label
        lambda_val = cut_lbls[0, 0].item()
    
    print(f"\n   üìä CutMix Pattern Analysis:")
    print(f"      White pixel ratios: min={min(cutmix_ratios):.3f}, max={max(cutmix_ratios):.3f}")
    print(f"      Mean ratio: {sum(cutmix_ratios)/len(cutmix_ratios):.3f}")
    
    # CutMix should produce different ratios each time
    unique_ratios = len(set([round(x, 2) for x in cutmix_ratios]))
    if unique_ratios > 1:
        print(f"   ‚úÖ CutMix produces varying cut sizes ({unique_ratios} unique ratios)")
    else:
        print(f"   ‚ö†Ô∏è  CutMix producing same cut size (may not be random)")
    
    # Test 5: Both MixUp and CutMix enabled (random switching)
    print(f"\nüîß TEST 7.5: MixUp + CutMix Together (Random Switching)")
    print("="*60)
    
    both_kwargs = {
        'mixup_alpha': 0.2,
        'cutmix_alpha': 1.0,
        'prob': 1.0,
        'switch_prob': 0.5,  # 50% chance of each
        'mode': 'batch',
        'label_smoothing': 0.1,
    }
    
    model_with_both = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES,
        mixup_kwargs=both_kwargs
    )
    
    if model_with_both.mixup_cutmix_fn is not None:
        print(f"   ‚úÖ Both MixUp and CutMix enabled")
        print(f"      MixUp alpha: {both_kwargs['mixup_alpha']}")
        print(f"      CutMix alpha: {both_kwargs['cutmix_alpha']}")
        print(f"      Switch probability: {both_kwargs['switch_prob']}")
    else:
        print(f"   ‚ùå Failed to enable both!")
        return False
    
    # Test with sample data
    test_batch = torch.randn(4, 3, 224, 224)
    test_labels_batch = torch.randint(0, NUM_CLASSES, (4,))
    
    mixed, mixed_lbl = model_with_both.mixup_cutmix_fn(test_batch, test_labels_batch)
    
    print(f"   ‚úÖ Successfully applied random MixUp/CutMix")
    print(f"      Output shape: {mixed.shape}")
    print(f"      Label shape: {mixed_lbl.shape}")
    
    # Test 6: Check configuration from config file
    print(f"\nüîß TEST 7.6: CutMix Configuration from Config")
    print("="*60)
    
    print(f"\n   Current CutMix configuration:")
    for key, value in MIXUP_KWARGS.items():
        print(f"      {key}: {value}")
    
    # Verify CutMix is enabled in config
    if MIXUP_KWARGS.get('cutmix_alpha', 0.0) > 0:
        print(f"   ‚úÖ CutMix is enabled in config (alpha={MIXUP_KWARGS['cutmix_alpha']})")
    else:
        print(f"   ‚ÑπÔ∏è  CutMix is disabled in config (alpha={MIXUP_KWARGS.get('cutmix_alpha', 0.0)})")
    
    # Check if both are enabled
    if MIXUP_KWARGS.get('mixup_alpha', 0.0) > 0 and MIXUP_KWARGS.get('cutmix_alpha', 0.0) > 0:
        print(f"   ‚úÖ Both MixUp and CutMix are enabled")
        print(f"      Switch probability: {MIXUP_KWARGS.get('switch_prob', 0.5)}")
    
    print("\n‚úÖ TEST 7 PASSED: CutMix is working correctly")
    return True

# ============================================================================
# TEST 8: Cosine Annealing Scheduler Verification
# ============================================================================
def test_cosine_annealing():
    """Verify Cosine Annealing LR scheduler configuration"""
    print("\n" + "="*80)
    print("TEST 8: COSINE ANNEALING SCHEDULER VERIFICATION")
    print("="*80)
    
    print(f"\nüìö Cosine Annealing Concept:")
    print(f"   - Gradually decreases learning rate following a cosine curve")
    print(f"   - Starts at initial LR and decreases to eta_min")
    print(f"   - Provides smooth, gradual decay over training")
    print(f"   - Formula: lr = eta_min + (initial_lr - eta_min) * (1 + cos(œÄ * t / T_max)) / 2")
    
    # Create a dummy model and optimizer
    model = ResnetLightningModule(
        learning_rate=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        num_classes=NUM_CLASSES
    )
    
    # Setup datamodule to get the dataloaders
    datamodule = ImageNetDataModule(
        train_img_dir=str(TRAIN_IMG_DIR),
        val_img_dir=str(VAL_IMG_DIR),
        mean=MEAN,
        std=STD,
        batch_size=BATCH_SIZE,
        num_workers=0,  # Use 0 workers for quick test
        initial_resolution=224
    )
    
    datamodule.setup('fit')
    
    # Calculate steps
    train_loader = datamodule.train_dataloader()
    steps_per_epoch = len(train_loader)
    total_steps = steps_per_epoch * EPOCHS
    
    print(f"\nüìä Training Configuration:")
    print(f"   Epochs: {EPOCHS}")
    print(f"   Batch size: {BATCH_SIZE}")
    print(f"   Train samples: {len(datamodule.train_dataset)}")
    print(f"   Steps per epoch: {steps_per_epoch}")
    print(f"   Total steps: {total_steps}")
    
    # Create optimizer
    optimizer = torch.optim.SGD(
        model.parameters(),
        lr=LEARNING_RATE,
        weight_decay=WEIGHT_DECAY,
        momentum=0.9
    )
    
    # Create Cosine Annealing scheduler
    eta_min = 1e-6
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer,
        T_max=total_steps,
        eta_min=eta_min
    )
    
    print(f"\nüîÑ Cosine Annealing Scheduler Configuration:")
    print(f"   Initial LR: {LEARNING_RATE:.4e}")
    print(f"   Minimum LR (eta_min): {eta_min:.4e}")
    print(f"   T_max (total steps): {total_steps}")
    print(f"   LR reduction ratio: {LEARNING_RATE / eta_min:.1f}x")
    
    # Simulate training and collect LR values
    print("\nüìà Simulating LR schedule over epochs...")
    lr_history = []
    
    for epoch in range(EPOCHS):
        for step in range(steps_per_epoch):
            lr_history.append(optimizer.param_groups[0]['lr'])
            scheduler.step()
    
    # Verify LR schedule characteristics
    print(f"\n‚úÖ LR Schedule Analysis:")
    print(f"   Starting LR: {lr_history[0]:.6e}")
    print(f"   Final LR: {lr_history[-1]:.6e}")
    print(f"   Minimum LR reached: {min(lr_history):.6e}")
    print(f"   Maximum LR: {max(lr_history):.6e}")
    
    # Check if LR follows expected pattern (monotonic decrease for cosine annealing)
    # Sample at quartiles
    q1_idx = len(lr_history) // 4
    q2_idx = len(lr_history) // 2
    q3_idx = 3 * len(lr_history) // 4
    
    print(f"\n   LR at different stages:")
    print(f"      25% progress: {lr_history[q1_idx]:.6e}")
    print(f"      50% progress: {lr_history[q2_idx]:.6e}")
    print(f"      75% progress: {lr_history[q3_idx]:.6e}")
    
    # Verify cosine decay pattern (should decrease monotonically for single cycle)
    is_decreasing = lr_history[0] > lr_history[q2_idx] > lr_history[-1]
    
    if is_decreasing:
        print(f"\n   ‚úÖ LR schedule follows correct cosine annealing pattern:")
        print(f"      - Starts high: {lr_history[0]:.6e}")
        print(f"      - Gradually decreases following cosine curve")
        print(f"      - Ends at minimum: {lr_history[-1]:.6e}")
    else:
        print(f"\n   ‚ùå WARNING: LR schedule may not be correct!")
    
    # Verify the decay is smooth (cosine should be smooth, not stepwise)
    # Check that consecutive LRs are close (smooth decay)
    lr_diffs = [abs(lr_history[i+1] - lr_history[i]) for i in range(0, len(lr_history)-1, 100)]
    max_diff = max(lr_diffs)
    mean_diff = sum(lr_diffs) / len(lr_diffs)
    
    print(f"\n   üìä Smoothness Analysis:")
    print(f"      Mean LR change per 100 steps: {mean_diff:.8e}")
    print(f"      Max LR change per 100 steps: {max_diff:.8e}")
    
    # The change should be gradual (not sudden jumps)
    if max_diff < LEARNING_RATE * 0.1:  # Max change should be less than 10% of initial LR
        print(f"   ‚úÖ LR decay is smooth (no sudden jumps)")
    else:
        print(f"   ‚ö†Ô∏è  LR decay may have sudden changes")
    
    # Test comparison with OneCycle
    print(f"\nüîÑ Comparison: Cosine Annealing vs OneCycle")
    print("="*60)
    
    print(f"\n   Cosine Annealing:")
    print(f"      - Monotonic decrease from start to end")
    print(f"      - No warmup phase")
    print(f"      - Smooth, predictable decay")
    print(f"      - Final LR: {lr_history[-1]:.6e}")
    
    print(f"\n   OneCycle (for reference):")
    print(f"      - Warmup phase (first 20%)")
    print(f"      - Peak LR in middle")
    print(f"      - Annealing phase (last 80%)")
    print(f"      - More aggressive LR changes")
    
    # Verify configuration from config
    print(f"\nüîß Configuration Check:")
    print("="*60)
    
    from configs.g5_config import SCHEDULER_TYPE
    
    print(f"   Scheduler type in config: {SCHEDULER_TYPE}")
    
    if SCHEDULER_TYPE == 'cosine_annealing':
        print(f"   ‚úÖ Cosine Annealing is configured as the scheduler")
    else:
        print(f"   ‚ÑπÔ∏è  Config uses: {SCHEDULER_TYPE}")
    
    print("\n‚úÖ TEST 8 PASSED: Cosine Annealing scheduler is working correctly")
    return True, lr_history

# ============================================================================
# MAIN EXECUTION
# ============================================================================
def main():
    """Run all verification tests"""
    print("\nüöÄ Starting verification of training components...")
    print(f"   Configuration: {PROFILE_NAME} - {PROFILE_DESCRIPTION}")
    print(f"   Dataset: {TRAIN_IMG_DIR}")
    
    results = {}
    
    try:
        # Test 1: Augmentations
        results['augmentations'] = test_augmentations()
    except Exception as e:
        print(f"\n‚ùå TEST 1 FAILED: {e}")
        results['augmentations'] = False
    
    try:
        # Test 2: OneCycle Policy
        results['onecycle'], lr_history, momentum_history = test_onecycle_policy()
    except Exception as e:
        print(f"\n‚ùå TEST 2 FAILED: {e}")
        results['onecycle'] = False
        lr_history, momentum_history = [], []
    
    try:
        # Test 3: Resolution Schedule
        results['resolution'] = test_resolution_schedule()
    except Exception as e:
        print(f"\n‚ùå TEST 3 FAILED: {e}")
        results['resolution'] = False
    
    try:
        # Test 4: BlurPool
        results['blurpool'] = test_blurpool()
    except Exception as e:
        print(f"\n‚ùå TEST 4 FAILED: {e}")
        results['blurpool'] = False
    
    try:
        # Test 5: FixRes
        results['fixres'] = test_fixres()
    except Exception as e:
        print(f"\n‚ùå TEST 5 FAILED: {e}")
        results['fixres'] = False
    
    try:
        # Test 6: MixUp
        results['mixup'] = test_mixup()
    except Exception as e:
        print(f"\n‚ùå TEST 6 FAILED: {e}")
        results['mixup'] = False
    
    try:
        # Test 7: CutMix
        results['cutmix'] = test_cutmix()
    except Exception as e:
        print(f"\n‚ùå TEST 7 FAILED: {e}")
        results['cutmix'] = False
    
    try:
        # Test 8: Cosine Annealing
        results['cosine_annealing'], cosine_lr_history = test_cosine_annealing()
    except Exception as e:
        print(f"\n‚ùå TEST 8 FAILED: {e}")
        results['cosine_annealing'] = False
        cosine_lr_history = []
    
    # Print summary
    print("\n" + "="*80)
    print("üìä VERIFICATION SUMMARY")
    print("="*80)
    
    for test_name, passed in results.items():
        status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
        print(f"   {test_name.upper()}: {status}")
    
    all_passed = all(results.values())
    
    if all_passed:
        print("\n" + "="*80)
        print("üéâ ALL TESTS PASSED! You're ready to train on GPU/AWS!")
        print("="*80)
    else:
        print("\n" + "="*80)
        print("‚ö†Ô∏è  SOME TESTS FAILED! Please review before training on GPU/AWS")
        print("="*80)
    
    return all_passed

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)

