# Training Logs - ImageNet ResNet-50 Training

**Target Accuracy:** 75% top-1 accuracy  
**Final Accuracy:** 77.45% top-1 accuracy âœ…  
**Training Completed:** 90 epochs  
**Total Training Time:** 356 minutes (~6 hours)

---

## Training Logs (Epoch 1 to 90)

### Epochs 1-61 (Initial Training)

| Epoch | Train Loss | Train Acc | Val Loss | Val Acc |
|-------|-----------|-----------|----------|---------|
| 1 | 6.4731 | 1.74% | 5.9321 | 4.10% |
| 2 | 6.0129 | 4.36% | 5.5839 | 6.11% |
| 3 | 5.7562 | 6.57% | 5.2395 | 8.47% |
| 4 | 5.5392 | 8.78% | 5.0507 | 10.41% |
| 5 | 5.3438 | 10.95% | 4.7927 | 12.34% |
| 6 | 5.1473 | 13.31% | 4.6555 | 13.88% |
| 7 | 4.9526 | 15.72% | 4.2816 | 17.74% |
| 8 | 4.7219 | 18.60% | 4.1750 | 18.73% |
| 9 | 4.5023 | 21.81% | 4.0129 | 20.65% |
| 10 | 4.2881 | 25.05% | 3.9588 | 21.46% |
| 11 | 4.0858 | 28.37% | 3.6580 | 26.00% |
| 12 | 3.9059 | 31.36% | 3.5473 | 28.70% |
| 13 | 3.7417 | 34.39% | 3.1581 | 33.19% |
| 14 | 3.5790 | 37.19% | 2.7737 | 39.52% |
| 15 | 3.4521 | 39.65% | 2.7909 | 39.70% |
| 16 | 3.3515 | 41.89% | 2.6618 | 42.93% |
| 17 | 3.2557 | 43.86% | 2.5799 | 43.89% |
| 18 | 3.1717 | 45.41% | 2.5473 | 45.35% |
| 19 | 3.0867 | 46.94% | 2.3051 | 49.07% |
| 20 | 3.0305 | 48.22% | 2.5090 | 45.60% |
| 21 | 2.9859 | 49.26% | 2.3207 | 49.50% |
| 22 | 2.9198 | 50.26% | 2.2199 | 50.56% |
| 23 | 2.8787 | 51.23% | 2.1720 | 52.75% |
| 24 | 2.8293 | 51.99% | 2.1530 | 52.55% |
| 25 | 2.7926 | 52.77% | 2.2128 | 50.59% |
| 26 | 2.7608 | 53.41% | 2.1183 | 52.53% |
| 27 | 2.7390 | 53.91% | 2.1438 | 53.54% |
| 28 | 2.7032 | 54.59% | 2.0961 | 52.77% |
| 29 | 2.6714 | 55.13% | 1.9477 | 55.51% |
| 30 | 2.6574 | 55.54% | 1.9930 | 56.04% |
| 31 | 2.6315 | 56.01% | 2.0067 | 55.15% |
| 32 | 2.6257 | 56.46% | 2.0699 | 54.17% |
| 33 | 2.5959 | 56.75% | 2.0511 | 54.73% |
| 34 | 2.5776 | 57.15% | 2.0456 | 55.22% |
| 35 | 2.5524 | 57.56% | 1.8177 | 59.04% |
| 36 | 2.5456 | 57.78% | 1.9823 | 56.61% |
| 37 | 2.5400 | 58.08% | 1.8786 | 57.64% |
| 38 | 2.5289 | 58.32% | 1.8351 | 58.17% |
| 39 | 2.5104 | 58.66% | 1.7929 | 60.41% |
| 40 | 2.5071 | 58.90% | 1.9779 | 57.53% |
| 41 | 2.4905 | 59.12% | 1.9583 | 56.30% |
| 42 | 2.4864 | 59.29% | 1.7497 | 59.76% |
| 43 | 2.4716 | 59.56% | 1.8011 | 59.00% |
| 44 | 2.4671 | 59.76% | 1.8131 | 59.45% |
| 45 | 2.4529 | 60.01% | 1.6654 | 61.33% |
| 46 | 2.4411 | 60.18% | 1.6509 | 61.58% |
| 47 | 2.4447 | 60.39% | 1.9062 | 57.60% |
| 48 | 2.4302 | 60.55% | 1.6599 | 61.76% |
| 49 | 2.4156 | 60.75% | 1.7080 | 61.04% |
| 50 | 2.4170 | 60.93% | 1.8163 | 59.17% |
| 51 | 2.4035 | 61.12% | 1.8033 | 60.35% |
| 52 | 2.3853 | 61.46% | 1.6313 | 62.33% |
| 53 | 2.3802 | 61.50% | 1.9178 | 57.41% |
| 54 | 2.3752 | 61.67% | 1.7122 | 61.56% |
| 55 | 2.3660 | 61.98% | 1.7640 | 60.07% |
| 56 | 2.3530 | 62.09% | 1.6224 | 63.87% |
| 57 | 2.3480 | 62.33% | 1.8986 | 58.57% |
| 58 | 2.3386 | 62.50% | 1.5806 | 64.07% |
| 59 | 2.3270 | 62.77% | 1.6234 | 63.31% |
| 60 | 2.3205 | 62.98% | 1.5458 | 64.12% |
| 61 | - | - | 1.3916 | **72.80%** |
| 62 | 2.3013 | 63.48% | 1.4579 | 65.78% |
| 63 | 2.2880 | 63.73% | 1.5589 | 64.70% |
| 64 | 2.2710 | 64.04% | 1.5819 | 64.58% |
| 65 | 2.2620 | 64.36% | 1.4581 | 66.32% |
| 66 | 2.2473 | 64.63% | 1.5324 | 64.66% |
| 67 | 2.2383 | 64.95% | 1.4381 | 66.57% |
| 68 | 2.2213 | 65.28% | 1.3809 | **68.09%** |
| 69 | 2.1980 | 65.72% | 1.4155 | 67.46% |
| 70 | 2.1870 | 66.18% | 1.4180 | 67.08% |
| 71 | 2.1576 | 66.57% | 1.3090 | 68.93% |
| 72 | 2.1423 | 67.15% | 1.3512 | 68.79% |
| 73 | 2.1198 | 67.64% | 1.2757 | 69.88% |
| 74 | 2.0946 | 68.20% | 1.2624 | 70.42% |
| 75 | 2.0737 | 68.80% | 1.2561 | 70.86% |
| 76 | 2.0388 | 69.42% | 1.2434 | 71.38% |
| 77 | 2.0216 | 70.03% | 1.1883 | 71.94% |
| 78 | 1.9901 | 70.74% | 1.2240 | 71.89% |
| 79 | 1.9624 | 71.50% | 1.1303 | 73.47% |
| 80 | 1.9254 | 72.19% | 1.1072 | **74.04%** âœ… |
| 81 | 1.8914 | 72.99% | 1.2447 | **75.07%** ğŸ¯ |
| 82 | 1.4159 | 84.79% | 0.9994 | **76.15%** â­ |
| 83 | 1.3601 | 86.26% | 0.9953 | **76.58%** â­ |
| 84 | 1.3255 | 87.40% | 0.9892 | **76.72%** â­ |
| 85 | 1.2910 | 88.42% | 0.9787 | **76.90%** â­ |
| 86 | 1.2607 | 89.23% | 0.9746 | **77.31%** â­ |
| 87 | 1.2461 | 89.94% | 0.9731 | 77.27% |
| 88 | 1.2202 | 90.40% | 0.9682 | 77.43% |
| 89 | 1.2115 | 90.74% | 0.9685 | **77.45%** ğŸ† |
| 90 | 1.2130 | 90.88% | 0.9716 | **77.45%** ğŸ† |

**Note:** Epochs 82-90 show a significant improvement, likely due to FixRes fine-tuning at higher resolution (288px). The model achieved the target of 75% accuracy at epoch 81 and continued improving to reach 77.45% at epochs 89-90.

---

## Key Milestones

- âœ… **Epoch 80:** Achieved **74.04%** - Just below target
- ğŸ¯ **Epoch 81:** Achieved **75.07%** - **TARGET REACHED!**
- â­ **Epoch 82:** **76.15%** - Continued improvement
- â­ **Epoch 86:** **77.31%** - Best single epoch result
- ğŸ† **Epoch 89-90:** **77.45%** - **FINAL BEST ACCURACY**

---

## Training Summary

### Final Results

- **Best Validation Accuracy:** 77.45% (Epochs 89-90)
- **Target Accuracy:** 75% âœ… **EXCEEDED by 2.45%**
- **Training Duration:** 129.7 minutes (~2.2 hours)
- **Total Epochs:** 90
- **Best Model Checkpoint:** Epoch 89

### Training Configuration

- **Model:** ResNet-50 with BlurPool
- **Total Parameters:** 25,576,264
- **Dataset:** ImageNet-1K (1.2M training images, 50K validation images)
- **Batch Size:** 512
- **Learning Rate:** 0.512 (max), 0.02048 (initial), 2.048e-05 (min)
- **Scheduler:** OneCycleLR
- **Optimizer:** SGD with momentum 0.95
- **Weight Decay:** 0.0001

### Training Schedule

| Epoch Range | Resolution | Augmentation |
|-------------|-----------|--------------|
| 0-10 | 128px | Train augmentations |
| 11-85 | 224px | Train augmentations |
| 86-90 | 288px | FixRes (Test-time augmentations) |

---

## Detailed Log Entries

### Epoch 1-61 (Initial Training Phase)

```
2025-11-01 20:09:45 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 1 VAL   - Loss: 6.9117, Acc: 0.00%, F1: N/A
2025-11-01 20:09:45 |     INFO | on_train_start:161 | ================================================================================
2025-11-01 20:09:45 |     INFO | on_train_start:162 | ğŸš€ Starting Lightning experiment: imagenet_p3_training
2025-11-01 20:09:45 |     INFO | on_train_start:163 | ğŸ“ Experiment directory: s3:/imagenet-resnet-50-erav4/data-4/imagenet_p3_training
2025-11-01 20:09:45 |     INFO | on_train_start:164 | ğŸ’¾ Log file: s3:/imagenet-resnet-50-erav4/data-4/imagenet_p3_training/training.log
2025-11-01 20:09:45 |     INFO | on_train_start:165 | ================================================================================
2025-11-01 20:09:45 |     INFO | _log_model_info:181 | ğŸ“‹ Detailed Model Summary:
2025-11-01 20:09:45 |     INFO | _log_model_info:182 | ------------------------------------------------------------
2025-11-01 20:09:45 |     INFO | _log_model_info:188 |    Total Parameters: 25,576,264
2025-11-01 20:09:45 |     INFO | _log_model_info:189 |    Trainable Parameters: 25,576,264
2025-11-01 20:09:45 |     INFO | _log_model_info:190 |    Non-trainable Parameters: 0
2025-11-01 20:09:45 |     INFO | _log_model_info:195 | ```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ResNet                                   [1, 1000]                 --
â”œâ”€Sequential: 1-1                        [1, 64, 128, 128]         --
â”‚    â””â”€Conv2d: 2-1                       [1, 32, 128, 128]         864
â”‚    â””â”€BatchNorm2d: 2-2                  [1, 32, 128, 128]         64
â”‚    â””â”€ReLU: 2-3                         [1, 32, 128, 128]         --
â”‚    â””â”€Conv2d: 2-4                       [1, 32, 128, 128]         9,216
â”‚    â””â”€BatchNorm2d: 2-5                  [1, 32, 128, 128]         64
â”‚    â””â”€ReLU: 2-6                         [1, 32, 128, 128]         --
â”‚    â””â”€Conv2d: 2-7                       [1, 64, 128, 128]         18,432
â”œâ”€BatchNorm2d: 1-2                       [1, 64, 128, 128]         128
â”œâ”€ReLU: 1-3                              [1, 64, 128, 128]         --
â”œâ”€MaxPool2d: 1-4                         [1, 64, 64, 64]           --
â”œâ”€Sequential: 1-5                        [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-8                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 64, 64, 64]           4,096
â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-3                    [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-4                  [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-5             [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-6                [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-7                    [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-8                [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-9                  [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-10            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€Sequential: 3-11             [1, 256, 64, 64]          16,896
â”‚    â”‚    â””â”€ReLU: 3-12                   [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-9                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 64, 64, 64]           16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-14            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-15                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-16                 [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-17            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-18               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-19                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-20               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-21                 [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-22            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-23                   [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-10                  [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-24                 [1, 64, 64, 64]           16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-25            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-26                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-27                 [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-28            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-29               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-30                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-31               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-32                 [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-33            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-34                   [1, 256, 64, 64]          --
â”œâ”€Sequential: 1-6                        [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-11                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-35                 [1, 128, 64, 64]          32,768
â”‚    â”‚    â””â”€BatchNorm2d: 3-36            [1, 128, 64, 64]          256
â”‚    â”‚    â””â”€ReLU: 3-37                   [1, 128, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-38                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-39            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-40               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-41                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-42               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-43                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-44            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€Sequential: 3-45             [1, 512, 32, 32]          132,096
â”‚    â”‚    â””â”€ReLU: 3-46                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-12                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-47                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-48            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-49                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-50                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-51            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-52               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-53                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-54               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-55                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-56            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-57                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-13                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-58                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-59            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-60                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-61                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-62            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-63               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-64                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-65               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-66                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-67            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-68                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-14                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-69                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-70            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-71                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-72                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-73            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-74               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-75                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-76               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-77                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-78            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-79                   [1, 512, 32, 32]          --
â”œâ”€Sequential: 1-7                        [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-15                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-80                 [1, 256, 32, 32]          131,072
â”‚    â”‚    â””â”€BatchNorm2d: 3-81            [1, 256, 32, 32]          512
â”‚    â”‚    â””â”€ReLU: 3-82                   [1, 256, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-83                 [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-84            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-85               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-86                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-87               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-88                 [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-89            [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€Sequential: 3-90             [1, 1024, 16, 16]         526,336
â”‚    â”‚    â””â”€ReLU: 3-91                   [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-16                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-92                 [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-93            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-94                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-95                 [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-96            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-97               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-98                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-99               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-100                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-101           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-102                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-17                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-103                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-104           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-105                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-106                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-107           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-108              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-109                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-110              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-111                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-112           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-113                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-18                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-114                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-115           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-116                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-117                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-118           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-119              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-120                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-121              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-122                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-123           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-124                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-19                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-125                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-126           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-127                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-128                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-129           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-130              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-131                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-132              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-133                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-134           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-135                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-20                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-136                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-137           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-138                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-139                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-140           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-141              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-142                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-143              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-144                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-145           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-146                  [1, 1024, 16, 16]         --
â”œâ”€Sequential: 1-8                        [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-21                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-147                [1, 512, 16, 16]          524,288
â”‚    â”‚    â””â”€BatchNorm2d: 3-148           [1, 512, 16, 16]          1,024
â”‚    â”‚    â””â”€ReLU: 3-149                  [1, 512, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-150                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-151           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-152              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-153                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-154              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-155                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-156           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€Sequential: 3-157            [1, 2048, 8, 8]           2,101,248
â”‚    â”‚    â””â”€ReLU: 3-158                  [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-22                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-159                [1, 512, 8, 8]            1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-160           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€ReLU: 3-161                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-162                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-163           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-164              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-165                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-166              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-167                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-168           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€ReLU: 3-169                  [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-23                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-170                [1, 512, 8, 8]            1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-171           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€ReLU: 3-172                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-173                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-174           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-175              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-176                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-177              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-178                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-179           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€ReLU: 3-180                  [1, 2048, 8, 8]           --
â”œâ”€SelectAdaptivePool2d: 1-9              [1, 2048]                 --
â”‚    â””â”€AdaptiveAvgPool2d: 2-24           [1, 2048, 1, 1]           --
â”‚    â””â”€Flatten: 2-25                     [1, 2048]                 --
â”œâ”€Linear: 1-10                           [1, 1000]                 2,049,000
==========================================================================================
Total params: 25,576,264
Trainable params: 25,576,264
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 5.65
==========================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 249.04
Params size (MB): 102.31
Estimated Total Size (MB): 352.14
==========================================================================================
2025-11-01 20:09:45 |     INFO | _log_training_config:234 | âš™ï¸ Training Configuration:
2025-11-01 20:09:45 |     INFO | _log_training_config:235 |    Device: cuda:0
2025-11-01 20:09:45 |     INFO | _log_training_config:236 |    Accelerator: <lightning.pytorch.accelerators.cuda.CUDAAccelerator object at 0x7f0f4824a1b0>
2025-11-01 20:09:45 |     INFO | _log_training_config:237 |    Max Epochs: 90
2025-11-01 20:09:45 |     INFO | _log_training_config:238 |    Learning Rate: 0.512
2025-11-01 20:09:45 |     INFO | _log_training_config:239 |    Weight Decay: 0.0001
2025-11-01 20:09:45 |     INFO | _log_training_config:272 | ğŸ”§ Optimizer Configuration:
2025-11-01 20:09:45 |     INFO | _log_training_config:273 |    Optimizer: SGD
2025-11-01 20:09:45 |     INFO | _log_training_config:277 |    Optimizer group 0:
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      lr: 0.020479999999999998
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      momentum: 0.95
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      dampening: 0
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      weight_decay: 0.0001
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      nesterov: False
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      maximize: False
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      foreach: None
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      differentiable: False
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      fused: None
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      initial_lr: 0.02048
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      max_lr: 0.512
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      min_lr: 2.048e-05
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      max_momentum: 0.95
2025-11-01 20:09:45 |     INFO | _log_training_config:280 |      base_momentum: 0.85
2025-11-01 20:09:45 |     INFO | _log_training_config:284 | ğŸ“… Learning Rate Scheduler Configuration:
2025-11-01 20:09:45 |     INFO | _log_training_config:294 |    Scheduler: OneCycleLR
2025-11-01 20:09:45 |     INFO | _log_training_config:295 |    Monitor: N/A
2025-11-01 20:09:45 |     INFO | _log_training_config:296 |    Frequency: 1
2025-11-01 20:09:45 |     INFO | _log_training_config:297 |    Interval: step
2025-11-01 20:09:45 |     INFO | _log_training_config:313 |    Scheduler Parameters:
2025-11-01 20:09:45 |     INFO | _log_training_config:315 |      total_steps: 28080
2025-11-01 20:09:45 |     INFO | _log_training_config:315 |      cycle_momentum: True
2025-11-01 20:09:45 |     INFO | _log_training_config:315 |      use_beta1: False
2025-11-01 20:09:45 |     INFO | _log_training_config:315 |      base_lrs: [0.02048]
2025-11-01 20:09:45 |     INFO | _log_training_config:315 |      last_epoch: 0
2025-11-01 20:09:45 |     INFO | _log_dataset_info:344 | ğŸ“Š Dataset Information:
2025-11-01 20:09:47 |     INFO | _log_dataset_info:352 |    Training batches: 2502
2025-11-01 20:09:47 |     INFO | _log_dataset_info:353 |    Validation batches: 97
2025-11-01 20:09:47 |     INFO | _log_dataset_info:354 |    Training samples: 1,281,167
2025-11-01 20:09:47 |     INFO | _log_dataset_info:355 |    Validation samples: 50,000
2025-11-01 20:09:47 |     INFO | _log_dataset_info:356 |    Batch size: 512
2025-11-01 20:09:48 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 1/90 - Starting...
2025-11-01 20:13:52 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 1 VAL   - Loss: 5.9321, Acc: 4.10%, F1: N/A
2025-11-01 20:13:53 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 1 TRAIN - Loss: 6.4731, Acc: 1.74%, F1: N/A
2025-11-01 20:14:01 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 2/90 - Starting...
2025-11-01 20:18:31 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 2 VAL   - Loss: 5.5839, Acc: 6.11%, F1: N/A
2025-11-01 20:18:31 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 2 TRAIN - Loss: 6.0129, Acc: 4.36%, F1: N/A
2025-11-01 20:18:39 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 3/90 - Starting...
2025-11-01 20:23:11 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 3 VAL   - Loss: 5.2395, Acc: 8.47%, F1: N/A
2025-11-01 20:23:12 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 3 TRAIN - Loss: 5.7562, Acc: 6.57%, F1: N/A
2025-11-01 20:23:19 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 4/90 - Starting...
2025-11-01 20:27:48 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 4 VAL   - Loss: 5.0507, Acc: 10.41%, F1: N/A
2025-11-01 20:27:49 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 4 TRAIN - Loss: 5.5392, Acc: 8.78%, F1: N/A
2025-11-01 20:27:57 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 5/90 - Starting...
2025-11-01 20:32:29 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 5 VAL   - Loss: 4.7927, Acc: 12.34%, F1: N/A
2025-11-01 20:32:29 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 5 TRAIN - Loss: 5.3438, Acc: 10.95%, F1: N/A
2025-11-01 20:32:37 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 6/90 - Starting...
2025-11-01 20:37:10 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 6 VAL   - Loss: 4.6555, Acc: 13.88%, F1: N/A
2025-11-01 20:37:11 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 6 TRAIN - Loss: 5.1473, Acc: 13.31%, F1: N/A
2025-11-01 20:37:19 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 7/90 - Starting...
2025-11-01 20:41:49 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 7 VAL   - Loss: 4.2816, Acc: 17.74%, F1: N/A
2025-11-01 20:41:50 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 7 TRAIN - Loss: 4.9526, Acc: 15.72%, F1: N/A
2025-11-01 20:41:58 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 8/90 - Starting...
2025-11-01 20:46:32 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 8 VAL   - Loss: 4.1750, Acc: 18.73%, F1: N/A
2025-11-01 20:46:33 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 8 TRAIN - Loss: 4.7219, Acc: 18.60%, F1: N/A
2025-11-01 20:46:41 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 9/90 - Starting...
2025-11-01 20:51:12 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 9 VAL   - Loss: 4.0129, Acc: 20.65%, F1: N/A
2025-11-01 20:51:13 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 9 TRAIN - Loss: 4.5023, Acc: 21.81%, F1: N/A
2025-11-01 20:51:20 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 10/90 - Starting...
2025-11-01 20:55:54 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 10 VAL   - Loss: 3.9588, Acc: 21.46%, F1: N/A
2025-11-01 20:55:54 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 10 TRAIN - Loss: 4.2881, Acc: 25.05%, F1: N/A
2025-11-01 20:56:02 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 11/90 - Starting...
2025-11-01 21:00:33 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 11 VAL   - Loss: 3.6580, Acc: 26.00%, F1: N/A
2025-11-01 21:00:33 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 11 TRAIN - Loss: 4.0858, Acc: 28.37%, F1: N/A
2025-11-01 21:00:41 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 12/90 - Starting...
2025-11-01 21:05:11 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 12 VAL   - Loss: 3.5473, Acc: 28.70%, F1: N/A
2025-11-01 21:05:12 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 12 TRAIN - Loss: 3.9059, Acc: 31.36%, F1: N/A
2025-11-01 21:05:20 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 13/90 - Starting...
2025-11-01 21:09:51 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 13 VAL   - Loss: 3.1581, Acc: 33.19%, F1: N/A
2025-11-01 21:09:52 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 13 TRAIN - Loss: 3.7417, Acc: 34.39%, F1: N/A
2025-11-01 21:09:59 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 14/90 - Starting...
2025-11-01 21:14:30 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 14 VAL   - Loss: 2.7737, Acc: 39.52%, F1: N/A
2025-11-01 21:14:31 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 14 TRAIN - Loss: 3.5790, Acc: 37.19%, F1: N/A
2025-11-01 21:14:38 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 15/90 - Starting...
2025-11-01 21:19:10 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 15 VAL   - Loss: 2.7909, Acc: 39.70%, F1: N/A
2025-11-01 21:19:10 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 15 TRAIN - Loss: 3.4521, Acc: 39.65%, F1: N/A
2025-11-01 21:19:19 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 16/90 - Starting...
2025-11-01 21:23:51 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 16 VAL   - Loss: 2.6618, Acc: 42.93%, F1: N/A
2025-11-01 21:23:52 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 16 TRAIN - Loss: 3.3515, Acc: 41.89%, F1: N/A
2025-11-01 21:24:00 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 17/90 - Starting...
2025-11-01 21:28:35 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 17 VAL   - Loss: 2.5799, Acc: 43.89%, F1: N/A
2025-11-01 21:28:36 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 17 TRAIN - Loss: 3.2557, Acc: 43.86%, F1: N/A
2025-11-01 21:28:44 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 18/90 - Starting...
2025-11-01 21:33:17 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 18 VAL   - Loss: 2.5473, Acc: 45.35%, F1: N/A
2025-11-01 21:33:18 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 18 TRAIN - Loss: 3.1717, Acc: 45.41%, F1: N/A
2025-11-01 21:33:26 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 19/90 - Starting...
2025-11-01 21:37:56 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 19 VAL   - Loss: 2.3051, Acc: 49.07%, F1: N/A
2025-11-01 21:37:57 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 19 TRAIN - Loss: 3.0867, Acc: 46.94%, F1: N/A
2025-11-01 21:38:09 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 20/90 - Starting...
2025-11-01 21:42:43 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 20 VAL   - Loss: 2.5090, Acc: 45.60%, F1: N/A
2025-11-01 21:42:44 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 20 TRAIN - Loss: 3.0305, Acc: 48.22%, F1: N/A
2025-11-01 21:42:49 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 21/90 - Starting...
2025-11-01 21:47:18 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 21 VAL   - Loss: 2.3207, Acc: 49.50%, F1: N/A
2025-11-01 21:47:18 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 21 TRAIN - Loss: 2.9859, Acc: 49.26%, F1: N/A
2025-11-01 21:47:26 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 22/90 - Starting...
2025-11-01 21:52:00 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 22 VAL   - Loss: 2.2199, Acc: 50.56%, F1: N/A
2025-11-01 21:52:01 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 22 TRAIN - Loss: 2.9198, Acc: 50.26%, F1: N/A
2025-11-01 21:52:08 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 23/90 - Starting...
2025-11-01 21:56:41 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 23 VAL   - Loss: 2.1720, Acc: 52.75%, F1: N/A
2025-11-01 21:56:41 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 23 TRAIN - Loss: 2.8787, Acc: 51.23%, F1: N/A
2025-11-01 21:56:49 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 24/90 - Starting...
2025-11-01 22:01:24 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 24 VAL   - Loss: 2.1530, Acc: 52.55%, F1: N/A
2025-11-01 22:01:24 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 24 TRAIN - Loss: 2.8293, Acc: 51.99%, F1: N/A
2025-11-01 22:01:29 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 25/90 - Starting...
2025-11-01 22:05:59 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 25 VAL   - Loss: 2.2128, Acc: 50.59%, F1: N/A
2025-11-01 22:05:59 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 25 TRAIN - Loss: 2.7926, Acc: 52.77%, F1: N/A
2025-11-01 22:06:05 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 26/90 - Starting...
2025-11-01 22:10:37 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 26 VAL   - Loss: 2.1183, Acc: 52.53%, F1: N/A
2025-11-01 22:10:38 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 26 TRAIN - Loss: 2.7608, Acc: 53.41%, F1: N/A
2025-11-01 22:10:43 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 27/90 - Starting...
2025-11-01 22:15:12 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 27 VAL   - Loss: 2.1438, Acc: 53.54%, F1: N/A
2025-11-01 22:15:13 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 27 TRAIN - Loss: 2.7390, Acc: 53.91%, F1: N/A
2025-11-01 22:15:20 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 28/90 - Starting...
2025-11-01 22:19:54 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 28 VAL   - Loss: 2.0961, Acc: 52.77%, F1: N/A
2025-11-01 22:19:55 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 28 TRAIN - Loss: 2.7032, Acc: 54.59%, F1: N/A
2025-11-01 22:20:00 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 29/90 - Starting...
2025-11-01 22:24:29 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 29 VAL   - Loss: 1.9477, Acc: 55.51%, F1: N/A
2025-11-01 22:24:30 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 29 TRAIN - Loss: 2.6714, Acc: 55.13%, F1: N/A
2025-11-01 22:24:38 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 30/90 - Starting...
2025-11-01 22:29:09 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 30 VAL   - Loss: 1.9930, Acc: 56.04%, F1: N/A
2025-11-01 22:29:10 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 30 TRAIN - Loss: 2.6574, Acc: 55.54%, F1: N/A
2025-11-01 22:29:18 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 31/90 - Starting...
2025-11-01 22:33:49 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 31 VAL   - Loss: 2.0067, Acc: 55.15%, F1: N/A
2025-11-01 22:33:50 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 31 TRAIN - Loss: 2.6315, Acc: 56.01%, F1: N/A
2025-11-01 22:33:55 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 32/90 - Starting...
2025-11-01 22:38:26 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 32 VAL   - Loss: 2.0699, Acc: 54.17%, F1: N/A
2025-11-01 22:38:26 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 32 TRAIN - Loss: 2.6257, Acc: 56.46%, F1: N/A
2025-11-01 22:38:31 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 33/90 - Starting...
2025-11-01 22:43:06 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 33 VAL   - Loss: 2.0511, Acc: 54.73%, F1: N/A
2025-11-01 22:43:07 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 33 TRAIN - Loss: 2.5959, Acc: 56.75%, F1: N/A
2025-11-01 22:43:12 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 34/90 - Starting...
2025-11-01 22:47:43 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 34 VAL   - Loss: 2.0456, Acc: 55.22%, F1: N/A
2025-11-01 22:47:44 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 34 TRAIN - Loss: 2.5776, Acc: 57.15%, F1: N/A
2025-11-01 22:47:49 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 35/90 - Starting...
2025-11-01 22:52:20 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 35 VAL   - Loss: 1.8177, Acc: 59.04%, F1: N/A
2025-11-01 22:52:21 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 35 TRAIN - Loss: 2.5524, Acc: 57.56%, F1: N/A
2025-11-01 22:52:28 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 36/90 - Starting...
2025-11-01 22:56:58 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 36 VAL   - Loss: 1.9823, Acc: 56.61%, F1: N/A
2025-11-01 22:56:58 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 36 TRAIN - Loss: 2.5456, Acc: 57.78%, F1: N/A
2025-11-01 22:57:03 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 37/90 - Starting...
2025-11-01 23:01:34 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 37 VAL   - Loss: 1.8786, Acc: 57.64%, F1: N/A
2025-11-01 23:01:34 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 37 TRAIN - Loss: 2.5400, Acc: 58.08%, F1: N/A
2025-11-01 23:01:40 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 38/90 - Starting...
2025-11-01 23:06:11 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 38 VAL   - Loss: 1.8351, Acc: 58.17%, F1: N/A
2025-11-01 23:06:11 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 38 TRAIN - Loss: 2.5289, Acc: 58.32%, F1: N/A
2025-11-01 23:06:16 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 39/90 - Starting...
2025-11-01 23:10:47 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 39 VAL   - Loss: 1.7929, Acc: 60.41%, F1: N/A
2025-11-01 23:10:47 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 39 TRAIN - Loss: 2.5104, Acc: 58.66%, F1: N/A
2025-11-01 23:10:59 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 40/90 - Starting...
2025-11-01 23:15:31 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 40 VAL   - Loss: 1.9779, Acc: 57.53%, F1: N/A
2025-11-01 23:15:32 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 40 TRAIN - Loss: 2.5071, Acc: 58.90%, F1: N/A
2025-11-01 23:15:37 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 41/90 - Starting...
2025-11-01 23:20:08 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 41 VAL   - Loss: 1.9583, Acc: 56.30%, F1: N/A
2025-11-01 23:20:08 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 41 TRAIN - Loss: 2.4905, Acc: 59.12%, F1: N/A
2025-11-01 23:20:13 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 42/90 - Starting...
2025-11-01 23:24:44 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 42 VAL   - Loss: 1.7497, Acc: 59.76%, F1: N/A
2025-11-01 23:24:45 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 42 TRAIN - Loss: 2.4864, Acc: 59.29%, F1: N/A
2025-11-01 23:24:50 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 43/90 - Starting...
2025-11-01 23:29:21 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 43 VAL   - Loss: 1.8011, Acc: 59.00%, F1: N/A
2025-11-01 23:29:22 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 43 TRAIN - Loss: 2.4716, Acc: 59.56%, F1: N/A
2025-11-01 23:29:27 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 44/90 - Starting...
2025-11-01 23:33:58 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 44 VAL   - Loss: 1.8131, Acc: 59.45%, F1: N/A
2025-11-01 23:33:58 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 44 TRAIN - Loss: 2.4671, Acc: 59.76%, F1: N/A
2025-11-01 23:34:03 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 45/90 - Starting...
2025-11-01 23:38:35 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 45 VAL   - Loss: 1.6654, Acc: 61.33%, F1: N/A
2025-11-01 23:38:36 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 45 TRAIN - Loss: 2.4529, Acc: 60.01%, F1: N/A
2025-11-01 23:38:44 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 46/90 - Starting...
2025-11-01 23:43:14 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 46 VAL   - Loss: 1.6509, Acc: 61.58%, F1: N/A
2025-11-01 23:43:15 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 46 TRAIN - Loss: 2.4411, Acc: 60.18%, F1: N/A
2025-11-01 23:43:22 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 47/90 - Starting...
2025-11-01 23:47:54 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 47 VAL   - Loss: 1.9062, Acc: 57.60%, F1: N/A
2025-11-01 23:47:55 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 47 TRAIN - Loss: 2.4447, Acc: 60.39%, F1: N/A
2025-11-01 23:48:00 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 48/90 - Starting...
2025-11-01 23:52:33 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 48 VAL   - Loss: 1.6599, Acc: 61.76%, F1: N/A
2025-11-01 23:52:33 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 48 TRAIN - Loss: 2.4302, Acc: 60.55%, F1: N/A
2025-11-01 23:52:41 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 49/90 - Starting...
2025-11-01 23:57:12 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 49 VAL   - Loss: 1.7080, Acc: 61.04%, F1: N/A
2025-11-01 23:57:13 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 49 TRAIN - Loss: 2.4156, Acc: 60.75%, F1: N/A
2025-11-01 23:57:20 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 50/90 - Starting...
2025-11-02 00:01:52 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 50 VAL   - Loss: 1.8163, Acc: 59.17%, F1: N/A
2025-11-02 00:01:52 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 50 TRAIN - Loss: 2.4170, Acc: 60.93%, F1: N/A
2025-11-02 00:01:58 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 51/90 - Starting...
2025-11-02 00:06:27 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 51 VAL   - Loss: 1.8033, Acc: 60.35%, F1: N/A
2025-11-02 00:06:28 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 51 TRAIN - Loss: 2.4035, Acc: 61.12%, F1: N/A
2025-11-02 00:06:33 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 52/90 - Starting...
2025-11-02 00:11:08 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 52 VAL   - Loss: 1.6313, Acc: 62.33%, F1: N/A
2025-11-02 00:11:09 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 52 TRAIN - Loss: 2.3853, Acc: 61.46%, F1: N/A
2025-11-02 00:11:17 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 53/90 - Starting...
2025-11-02 00:15:48 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 53 VAL   - Loss: 1.9178, Acc: 57.41%, F1: N/A
2025-11-02 00:15:49 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 53 TRAIN - Loss: 2.3802, Acc: 61.50%, F1: N/A
2025-11-02 00:15:59 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 54/90 - Starting...
2025-11-02 00:20:31 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 54 VAL   - Loss: 1.7122, Acc: 61.56%, F1: N/A
2025-11-02 00:20:32 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 54 TRAIN - Loss: 2.3752, Acc: 61.67%, F1: N/A
2025-11-02 00:20:37 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 55/90 - Starting...
2025-11-02 00:25:12 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 55 VAL   - Loss: 1.7640, Acc: 60.07%, F1: N/A
2025-11-02 00:25:13 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 55 TRAIN - Loss: 2.3660, Acc: 61.98%, F1: N/A
2025-11-02 00:25:18 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 56/90 - Starting...
2025-11-02 00:29:51 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 56 VAL   - Loss: 1.6224, Acc: 63.87%, F1: N/A
2025-11-02 00:29:52 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 56 TRAIN - Loss: 2.3530, Acc: 62.09%, F1: N/A
2025-11-02 00:30:00 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 57/90 - Starting...
2025-11-02 00:34:33 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 57 VAL   - Loss: 1.8986, Acc: 58.57%, F1: N/A
2025-11-02 00:34:34 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 57 TRAIN - Loss: 2.3480, Acc: 62.33%, F1: N/A
2025-11-02 00:34:39 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 58/90 - Starting...
2025-11-02 00:39:07 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 58 VAL   - Loss: 1.5806, Acc: 64.07%, F1: N/A
2025-11-02 00:39:08 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 58 TRAIN - Loss: 2.3386, Acc: 62.50%, F1: N/A
2025-11-02 00:39:15 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 59/90 - Starting...
2025-11-02 00:43:49 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 59 VAL   - Loss: 1.6234, Acc: 63.31%, F1: N/A
2025-11-02 00:43:49 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 59 TRAIN - Loss: 2.3270, Acc: 62.77%, F1: N/A
2025-11-02 00:43:54 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 60/90 - Starting...
2025-11-02 00:48:28 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 60 VAL   - Loss: 1.5458, Acc: 64.12%, F1: N/A
2025-11-02 00:48:28 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 60 TRAIN - Loss: 2.3205, Acc: 62.98%, F1: N/A
2025-11-02 00:55:35 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 61 VAL   - Loss: 1.3916, Acc: 72.80%, F1: N/A
```

### Epoch 62-90 (Resumed Training Phase)

```
2025-11-02 00:55:37 |     INFO | on_train_start:161 | ================================================================================
2025-11-02 00:55:37 |     INFO | on_train_start:162 | ğŸš€ Starting Lightning experiment: imagenet_p3_training
2025-11-02 00:55:37 |     INFO | on_train_start:163 | ğŸ“ Experiment directory: s3:/imagenet-resnet-50-erav4/data-4/imagenet_p3_training
2025-11-02 00:55:37 |     INFO | on_train_start:164 | ğŸ’¾ Log file: s3:/imagenet-resnet-50-erav4/data-4/imagenet_p3_training/training.log
2025-11-02 00:55:37 |     INFO | on_train_start:165 | ================================================================================
2025-11-02 00:55:37 |     INFO | _log_model_info:181 | ğŸ“‹ Detailed Model Summary:
2025-11-02 00:55:37 |     INFO | _log_model_info:182 | ------------------------------------------------------------
2025-11-02 00:55:37 |     INFO | _log_model_info:188 |    Total Parameters: 25,576,264
2025-11-02 00:55:37 |     INFO | _log_model_info:189 |    Trainable Parameters: 25,576,264
2025-11-02 00:55:37 |     INFO | _log_model_info:190 |    Non-trainable Parameters: 0
2025-11-02 00:55:37 |     INFO | _log_model_info:195 | ```
==========================================================================================
Layer (type:depth-idx)                   Output Shape              Param #
==========================================================================================
ResNet                                   [1, 1000]                 --
â”œâ”€Sequential: 1-1                        [1, 64, 128, 128]         --
â”‚    â””â”€Conv2d: 2-1                       [1, 32, 128, 128]         864
â”‚    â””â”€BatchNorm2d: 2-2                  [1, 32, 128, 128]         64
â”‚    â””â”€ReLU: 2-3                         [1, 32, 128, 128]         --
â”‚    â””â”€Conv2d: 2-4                       [1, 32, 128, 128]         9,216
â”‚    â””â”€BatchNorm2d: 2-5                  [1, 32, 128, 128]         64
â”‚    â””â”€ReLU: 2-6                         [1, 32, 128, 128]         --
â”‚    â””â”€Conv2d: 2-7                       [1, 64, 128, 128]         18,432
â”œâ”€BatchNorm2d: 1-2                       [1, 64, 128, 128]         128
â”œâ”€ReLU: 1-3                              [1, 64, 128, 128]         --
â”œâ”€MaxPool2d: 1-4                         [1, 64, 64, 64]           --
â”œâ”€Sequential: 1-5                        [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-8                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-1                  [1, 64, 64, 64]           4,096
â”‚    â”‚    â””â”€BatchNorm2d: 3-2             [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-3                    [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-4                  [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-5             [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-6                [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-7                    [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-8                [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-9                  [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-10            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€Sequential: 3-11             [1, 256, 64, 64]          16,896
â”‚    â”‚    â””â”€ReLU: 3-12                   [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-9                   [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-13                 [1, 64, 64, 64]           16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-14            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-15                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-16                 [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-17            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-18               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-19                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-20               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-21                 [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-22            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-23                   [1, 256, 64, 64]          --
â”‚    â””â”€Bottleneck: 2-10                  [1, 256, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-24                 [1, 64, 64, 64]           16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-25            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€ReLU: 3-26                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-27                 [1, 64, 64, 64]           36,864
â”‚    â”‚    â””â”€BatchNorm2d: 3-28            [1, 64, 64, 64]           128
â”‚    â”‚    â””â”€Identity: 3-29               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€ReLU: 3-30                   [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Identity: 3-31               [1, 64, 64, 64]           --
â”‚    â”‚    â””â”€Conv2d: 3-32                 [1, 256, 64, 64]          16,384
â”‚    â”‚    â””â”€BatchNorm2d: 3-33            [1, 256, 64, 64]          512
â”‚    â”‚    â””â”€ReLU: 3-34                   [1, 256, 64, 64]          --
â”œâ”€Sequential: 1-6                        [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-11                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-35                 [1, 128, 64, 64]          32,768
â”‚    â”‚    â””â”€BatchNorm2d: 3-36            [1, 128, 64, 64]          256
â”‚    â”‚    â””â”€ReLU: 3-37                   [1, 128, 64, 64]          --
â”‚    â”‚    â””â”€Conv2d: 3-38                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-39            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-40               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-41                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-42               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-43                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-44            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€Sequential: 3-45             [1, 512, 32, 32]          132,096
â”‚    â”‚    â””â”€ReLU: 3-46                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-12                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-47                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-48            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-49                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-50                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-51            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-52               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-53                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-54               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-55                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-56            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-57                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-13                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-58                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-59            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-60                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-61                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-62            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-63               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-64                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-65               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-66                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-67            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-68                   [1, 512, 32, 32]          --
â”‚    â””â”€Bottleneck: 2-14                  [1, 512, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-69                 [1, 128, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-70            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€ReLU: 3-71                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-72                 [1, 128, 32, 32]          147,456
â”‚    â”‚    â””â”€BatchNorm2d: 3-73            [1, 128, 32, 32]          256
â”‚    â”‚    â””â”€Identity: 3-74               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€ReLU: 3-75                   [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Identity: 3-76               [1, 128, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-77                 [1, 512, 32, 32]          65,536
â”‚    â”‚    â””â”€BatchNorm2d: 3-78            [1, 512, 32, 32]          1,024
â”‚    â”‚    â””â”€ReLU: 3-79                   [1, 512, 32, 32]          --
â”œâ”€Sequential: 1-7                        [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-15                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-80                 [1, 256, 32, 32]          131,072
â”‚    â”‚    â””â”€BatchNorm2d: 3-81            [1, 256, 32, 32]          512
â”‚    â”‚    â””â”€ReLU: 3-82                   [1, 256, 32, 32]          --
â”‚    â”‚    â””â”€Conv2d: 3-83                 [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-84            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-85               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-86                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-87               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-88                 [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-89            [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€Sequential: 3-90             [1, 1024, 16, 16]         526,336
â”‚    â”‚    â””â”€ReLU: 3-91                   [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-16                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-92                 [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-93            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-94                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-95                 [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-96            [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-97               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-98                   [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-99               [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-100                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-101           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-102                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-17                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-103                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-104           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-105                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-106                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-107           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-108              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-109                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-110              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-111                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-112           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-113                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-18                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-114                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-115           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-116                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-117                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-118           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-119              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-120                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-121              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-122                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-123           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-124                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-19                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-125                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-126           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-127                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-128                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-129           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-130              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-131                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-132              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-133                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-134           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-135                  [1, 1024, 16, 16]         --
â”‚    â””â”€Bottleneck: 2-20                  [1, 1024, 16, 16]         --
â”‚    â”‚    â””â”€Conv2d: 3-136                [1, 256, 16, 16]          262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-137           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€ReLU: 3-138                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-139                [1, 256, 16, 16]          589,824
â”‚    â”‚    â””â”€BatchNorm2d: 3-140           [1, 256, 16, 16]          512
â”‚    â”‚    â””â”€Identity: 3-141              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€ReLU: 3-142                  [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Identity: 3-143              [1, 256, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-144                [1, 1024, 16, 16]         262,144
â”‚    â”‚    â””â”€BatchNorm2d: 3-145           [1, 1024, 16, 16]         2,048
â”‚    â”‚    â””â”€ReLU: 3-146                  [1, 1024, 16, 16]         --
â”œâ”€Sequential: 1-8                        [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-21                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-147                [1, 512, 16, 16]          524,288
â”‚    â”‚    â””â”€BatchNorm2d: 3-148           [1, 512, 16, 16]          1,024
â”‚    â”‚    â””â”€ReLU: 3-149                  [1, 512, 16, 16]          --
â”‚    â”‚    â””â”€Conv2d: 3-150                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-151           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-152              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-153                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-154              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-155                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-156           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€Sequential: 3-157            [1, 2048, 8, 8]           2,101,248
â”‚    â”‚    â””â”€ReLU: 3-158                  [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-22                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-159                [1, 512, 8, 8]            1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-160           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€ReLU: 3-161                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-162                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-163           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-164              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-165                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-166              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-167                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-168           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€ReLU: 3-169                  [1, 2048, 8, 8]           --
â”‚    â””â”€Bottleneck: 2-23                  [1, 2048, 8, 8]           --
â”‚    â”‚    â””â”€Conv2d: 3-170                [1, 512, 8, 8]            1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-171           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€ReLU: 3-172                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-173                [1, 512, 8, 8]            2,359,296
â”‚    â”‚    â””â”€BatchNorm2d: 3-174           [1, 512, 8, 8]            1,024
â”‚    â”‚    â””â”€Identity: 3-175              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€ReLU: 3-176                  [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Identity: 3-177              [1, 512, 8, 8]            --
â”‚    â”‚    â””â”€Conv2d: 3-178                [1, 2048, 8, 8]           1,048,576
â”‚    â”‚    â””â”€BatchNorm2d: 3-179           [1, 2048, 8, 8]           4,096
â”‚    â”‚    â””â”€ReLU: 3-180                  [1, 2048, 8, 8]           --
â”œâ”€SelectAdaptivePool2d: 1-9              [1, 2048]                 --
â”‚    â””â”€AdaptiveAvgPool2d: 2-24           [1, 2048, 1, 1]           --
â”‚    â””â”€Flatten: 2-25                     [1, 2048]                 --
â”œâ”€Linear: 1-10                           [1, 1000]                 2,049,000
==========================================================================================
Total params: 25,576,264
Trainable params: 25,576,264
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 5.65
==========================================================================================
Input size (MB): 0.79
Forward/backward pass size (MB): 249.04
Params size (MB): 102.31
Estimated Total Size (MB): 352.14
==========================================================================================

2025-11-02 00:55:37 |     INFO | _log_training_config:234 | âš™ï¸ Training Configuration:
2025-11-02 00:55:37 |     INFO | _log_training_config:235 |    Device: cuda:0
2025-11-02 00:55:37 |     INFO | _log_training_config:236 |    Accelerator: <lightning.pytorch.accelerators.cuda.CUDAAccelerator object at 0x7fd088c51130>
2025-11-02 00:55:37 |     INFO | _log_training_config:237 |    Max Epochs: 90
2025-11-02 00:55:37 |     INFO | _log_training_config:238 |    Learning Rate: 0.512
2025-11-02 00:55:37 |     INFO | _log_training_config:239 |    Weight Decay: 0.0001
2025-11-02 00:55:37 |     INFO | _log_training_config:272 | ğŸ”§ Optimizer Configuration:
2025-11-02 00:55:37 |     INFO | _log_training_config:273 |    Optimizer: SGD
2025-11-02 00:55:37 |     INFO | _log_training_config:277 |    Optimizer group 0:
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      lr: 0.020479999999999998
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      momentum: 0.95
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      dampening: 0
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      weight_decay: 0.0001
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      nesterov: False
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      maximize: False
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      foreach: None
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      differentiable: False
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      fused: None
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      initial_lr: 0.02048
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      max_lr: 0.512
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      min_lr: 2.048e-05
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      max_momentum: 0.95
2025-11-02 00:55:37 |     INFO | _log_training_config:280 |      base_momentum: 0.85
2025-11-02 00:55:37 |     INFO | _log_training_config:284 | ğŸ“… Learning Rate Scheduler Configuration:
2025-11-02 00:55:37 |     INFO | _log_training_config:294 |    Scheduler: OneCycleLR
2025-11-02 00:55:37 |     INFO | _log_training_config:295 |    Monitor: N/A
2025-11-02 00:55:37 |     INFO | _log_training_config:296 |    Frequency: 1
2025-11-02 00:55:37 |     INFO | _log_training_config:297 |    Interval: step
2025-11-02 00:55:37 |     INFO | _log_training_config:313 |    Scheduler Parameters:
2025-11-02 00:55:37 |     INFO | _log_training_config:315 |      total_steps: 28080
2025-11-02 00:55:37 |     INFO | _log_training_config:315 |      cycle_momentum: True
2025-11-02 00:55:37 |     INFO | _log_training_config:315 |      use_beta1: False
2025-11-02 00:55:37 |     INFO | _log_training_config:315 |      base_lrs: [0.02048]
2025-11-02 00:55:37 |     INFO | _log_training_config:315 |      last_epoch: 0
2025-11-02 00:55:37 |     INFO | _log_dataset_info:344 | ğŸ“Š Dataset Information:
2025-11-02 00:55:41 |     INFO | _log_dataset_info:352 |    Training batches: 2502
2025-11-02 00:55:41 |     INFO | _log_dataset_info:353 |    Validation batches: 97
2025-11-02 00:55:41 |     INFO | _log_dataset_info:354 |    Training samples: 1,281,167
2025-11-02 00:55:41 |     INFO | _log_dataset_info:355 |    Validation samples: 50,000
2025-11-02 00:55:41 |     INFO | _log_dataset_info:356 |    Batch size: 512
2025-11-02 00:55:50 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 62/90 - Starting...
2025-11-02 01:00:08 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 62 VAL   - Loss: 1.4579, Acc: 65.78%, F1: N/A
2025-11-02 01:00:08 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 62 TRAIN - Loss: 2.3013, Acc: 63.48%, F1: N/A
2025-11-02 01:00:16 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 63/90 - Starting...
2025-11-02 01:04:45 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 63 VAL   - Loss: 1.5589, Acc: 64.70%, F1: N/A
2025-11-02 01:04:46 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 63 TRAIN - Loss: 2.2880, Acc: 63.73%, F1: N/A
2025-11-02 01:04:51 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 64/90 - Starting...
2025-11-02 01:09:24 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 64 VAL   - Loss: 1.5819, Acc: 64.58%, F1: N/A
2025-11-02 01:09:25 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 64 TRAIN - Loss: 2.2710, Acc: 64.04%, F1: N/A
2025-11-02 01:09:35 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 65/90 - Starting...
2025-11-02 01:14:07 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 65 VAL   - Loss: 1.4581, Acc: 66.32%, F1: N/A
2025-11-02 01:14:08 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 65 TRAIN - Loss: 2.2620, Acc: 64.36%, F1: N/A
2025-11-02 01:14:17 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 66/90 - Starting...
2025-11-02 01:18:49 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 66 VAL   - Loss: 1.5324, Acc: 64.66%, F1: N/A
2025-11-02 01:18:50 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 66 TRAIN - Loss: 2.2473, Acc: 64.63%, F1: N/A
2025-11-02 01:18:55 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 67/90 - Starting...
2025-11-02 01:23:28 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 67 VAL   - Loss: 1.4381, Acc: 66.57%, F1: N/A
2025-11-02 01:23:28 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 67 TRAIN - Loss: 2.2383, Acc: 64.95%, F1: N/A
2025-11-02 01:23:36 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 68/90 - Starting...
2025-11-02 01:28:09 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 68 VAL   - Loss: 1.3809, Acc: 68.09%, F1: N/A
2025-11-02 01:28:09 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 68 TRAIN - Loss: 2.2213, Acc: 65.28%, F1: N/A
2025-11-02 01:28:17 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 69/90 - Starting...
2025-11-02 01:32:48 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 69 VAL   - Loss: 1.4155, Acc: 67.46%, F1: N/A
2025-11-02 01:32:49 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 69 TRAIN - Loss: 2.1980, Acc: 65.72%, F1: N/A
2025-11-02 01:32:54 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 70/90 - Starting...
2025-11-02 01:37:26 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 70 VAL   - Loss: 1.4180, Acc: 67.08%, F1: N/A
2025-11-02 01:37:26 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 70 TRAIN - Loss: 2.1870, Acc: 66.18%, F1: N/A
2025-11-02 01:37:32 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 71/90 - Starting...
2025-11-02 01:42:04 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 71 VAL   - Loss: 1.3090, Acc: 68.93%, F1: N/A
2025-11-02 01:42:04 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 71 TRAIN - Loss: 2.1576, Acc: 66.57%, F1: N/A
2025-11-02 01:42:12 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 72/90 - Starting...
2025-11-02 01:46:45 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 72 VAL   - Loss: 1.3512, Acc: 68.79%, F1: N/A
2025-11-02 01:46:45 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 72 TRAIN - Loss: 2.1423, Acc: 67.15%, F1: N/A
2025-11-02 01:46:50 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 73/90 - Starting...
2025-11-02 01:51:22 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 73 VAL   - Loss: 1.2757, Acc: 69.88%, F1: N/A
2025-11-02 01:51:23 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 73 TRAIN - Loss: 2.1198, Acc: 67.64%, F1: N/A
2025-11-02 01:51:31 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 74/90 - Starting...
2025-11-02 01:56:02 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 74 VAL   - Loss: 1.2624, Acc: 70.42%, F1: N/A
2025-11-02 01:56:02 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 74 TRAIN - Loss: 2.0946, Acc: 68.20%, F1: N/A
2025-11-02 01:56:10 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 75/90 - Starting...
2025-11-02 02:00:43 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 75 VAL   - Loss: 1.2561, Acc: 70.86%, F1: N/A
2025-11-02 02:00:44 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 75 TRAIN - Loss: 2.0737, Acc: 68.80%, F1: N/A
2025-11-02 02:00:52 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 76/90 - Starting...
2025-11-02 02:05:25 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 76 VAL   - Loss: 1.2434, Acc: 71.38%, F1: N/A
2025-11-02 02:05:25 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 76 TRAIN - Loss: 2.0388, Acc: 69.42%, F1: N/A
2025-11-02 02:05:33 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 77/90 - Starting...
2025-11-02 02:10:05 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 77 VAL   - Loss: 1.1883, Acc: 71.94%, F1: N/A
2025-11-02 02:10:06 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 77 TRAIN - Loss: 2.0216, Acc: 70.03%, F1: N/A
2025-11-02 02:10:13 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 78/90 - Starting...
2025-11-02 02:14:46 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 78 VAL   - Loss: 1.2240, Acc: 71.89%, F1: N/A
2025-11-02 02:14:47 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 78 TRAIN - Loss: 1.9901, Acc: 70.74%, F1: N/A
2025-11-02 02:14:52 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 79/90 - Starting...
2025-11-02 02:19:27 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 79 VAL   - Loss: 1.1303, Acc: 73.47%, F1: N/A
2025-11-02 02:19:28 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 79 TRAIN - Loss: 1.9624, Acc: 71.50%, F1: N/A
2025-11-02 02:19:35 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 80/90 - Starting...
2025-11-02 02:24:09 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 80 VAL   - Loss: 1.1072, Acc: 74.04%, F1: N/A
2025-11-02 02:24:09 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 80 TRAIN - Loss: 1.9254, Acc: 72.19%, F1: N/A
2025-11-02 02:24:17 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 81/90 - Starting...
2025-11-02 02:28:52 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 81 VAL   - Loss: 1.2447, Acc: 75.07%, F1: N/A
2025-11-02 02:28:53 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 81 TRAIN - Loss: 1.8914, Acc: 72.99%, F1: N/A
2025-11-02 02:29:01 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 82/90 - Starting...
2025-11-02 02:32:54 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 82 VAL   - Loss: 0.9994, Acc: 76.15%, F1: N/A
2025-11-02 02:32:55 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 82 TRAIN - Loss: 1.4159, Acc: 84.79%, F1: N/A
2025-11-02 02:33:03 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 83/90 - Starting...
2025-11-02 02:36:57 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 83 VAL   - Loss: 0.9953, Acc: 76.58%, F1: N/A
2025-11-02 02:36:57 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 83 TRAIN - Loss: 1.3601, Acc: 86.26%, F1: N/A
2025-11-02 02:37:05 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 84/90 - Starting...
2025-11-02 02:40:59 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 84 VAL   - Loss: 0.9892, Acc: 76.72%, F1: N/A
2025-11-02 02:41:00 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 84 TRAIN - Loss: 1.3255, Acc: 87.40%, F1: N/A
2025-11-02 02:41:07 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 85/90 - Starting...
2025-11-02 02:45:04 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 85 VAL   - Loss: 0.9787, Acc: 76.90%, F1: N/A
2025-11-02 02:45:04 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 85 TRAIN - Loss: 1.2910, Acc: 88.42%, F1: N/A
2025-11-02 02:45:12 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 86/90 - Starting...
2025-11-02 02:49:04 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 86 VAL   - Loss: 0.9746, Acc: 77.31%, F1: N/A
2025-11-02 02:49:05 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 86 TRAIN - Loss: 1.2607, Acc: 89.23%, F1: N/A
2025-11-02 02:49:13 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 87/90 - Starting...
2025-11-02 02:53:04 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 87 VAL   - Loss: 0.9731, Acc: 77.27%, F1: N/A
2025-11-02 02:53:04 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 87 TRAIN - Loss: 1.2461, Acc: 89.94%, F1: N/A
2025-11-02 02:53:10 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 88/90 - Starting...
2025-11-02 02:57:02 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 88 VAL   - Loss: 0.9682, Acc: 77.43%, F1: N/A
2025-11-02 02:57:03 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 88 TRAIN - Loss: 1.2202, Acc: 90.40%, F1: N/A
2025-11-02 02:57:11 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 89/90 - Starting...
2025-11-02 03:01:05 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 89 VAL   - Loss: 0.9685, Acc: 77.42%, F1: N/A
2025-11-02 03:01:06 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 89 TRAIN - Loss: 1.2115, Acc: 90.74%, F1: N/A
2025-11-02 03:01:11 |     INFO | on_train_epoch_start:364 | ğŸ”„ EPOCH 90/90 - Starting...
2025-11-02 03:05:11 |     INFO | on_validation_epoch_end:413 | ğŸ“Š EPOCH 90 VAL   - Loss: 0.9716, Acc: 77.45%, F1: N/A
2025-11-02 03:05:11 |     INFO | on_train_epoch_end:383 | ğŸ“ˆ EPOCH 90 TRAIN - Loss: 1.2130, Acc: 90.88%, F1: N/A
2025-11-02 03:05:17 |     INFO | on_train_end:470 | ================================================================================
2025-11-02 03:05:17 |     INFO | on_train_end:471 | âœ… TRAINING COMPLETED
2025-11-02 03:05:17 |     INFO | on_train_end:472 | â±ï¸  Total Duration: 7780.05 seconds (129.7 minutes)
2025-11-02 03:05:17 |     INFO | on_train_end:473 | ğŸ“… End Time: 2025-11-02 03:05:17
2025-11-02 03:05:17 |     INFO | on_train_end:474 | ğŸ Final Epoch: 91
2025-11-02 03:05:17 |     INFO | on_train_end:479 | ğŸ† Best Model: s3:/imagenet-resnet-50-erav4/data-4/imagenet_p3_training-checkpoints-1762044897.2790077/imagenet1k-epoch=89-val/accuracy=0.774.ckpt
2025-11-02 03:05:17 |     INFO | on_train_end:480 | ğŸ“Š Best Score: 0.77447509765625
2025-11-02 03:05:17 |     INFO | on_train_end:483 | ================================================================================
2025-11-02 03:05:17 |     INFO | _save_metrics_to_json:564 | ğŸ’¾ Metrics saved to: s3://imagenet-resnet-50-erav4/data-4/imagenet_p3_training/metrics.json
```

### Training Completion

```
2025-11-02 03:05:17 | INFO | on_train_end | ================================================================================
2025-11-02 03:05:17 | INFO | on_train_end | âœ… TRAINING COMPLETED
2025-11-02 03:05:17 | INFO | on_train_end | â±ï¸  Total Duration: 7780.05 seconds (129.7 minutes)
2025-11-02 03:05:17 | INFO | on_train_end | ğŸ“… End Time: 2025-11-02 03:05:17
2025-11-02 03:05:17 | INFO | on_train_end | ğŸ Final Epoch: 91
2025-11-02 03:05:17 | INFO | on_train_end | ğŸ† Best Model: imagenet1k-epoch=89-val/accuracy=0.774.ckpt
2025-11-02 03:05:17 | INFO | on_train_end | ğŸ“Š Best Score: 0.77447509765625 (77.45%)
```

---

## Performance Analysis

### Accuracy Progression

The training showed consistent improvement throughout:
- **Early Phase (Epochs 1-30):** Rapid learning from 4.10% to 56.04%
- **Middle Phase (Epochs 31-60):** Steady improvement from 56% to 64%
- **Late Phase (Epochs 61-80):** Continued refinement approaching target
- **FixRes Phase (Epochs 81-90):** Final push exceeding target, reaching 77.45%

### Loss Trends

- **Training Loss:** Decreased from 6.47 to 1.21 (81% reduction)
- **Validation Loss:** Decreased from 5.93 to 0.97 (84% reduction)
- **Generalization:** Good alignment between train and validation metrics

---

**Training completed successfully with target exceeded!** ğŸ‰

