{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0aaf2435",
   "metadata": {},
   "source": [
    "# ImageNet Training - Medium Recipe (Local) üå∂Ô∏èüå∂Ô∏è\n",
    "\n",
    "**Target Hardware:** MacBook M4 Pro (Apple Silicon - MPS)\n",
    "\n",
    "**Training Time:** Longer on single GPU (good for ImageNet-Mini)  \n",
    "**Target Accuracy:** 78.1% - 79.5%\n",
    "\n",
    "## Implemented Techniques:\n",
    "\n",
    "### Speed Up Methods:\n",
    "1. ‚úÖ **BlurPool** - Antialiased downsampling for shift-invariance\n",
    "2. ‚úÖ **FixRes** - Fine-tuning at higher resolution than training\n",
    "3. ‚úÖ **Label Smoothing** - Prevents overconfident predictions (0.1 smoothing)\n",
    "4. ‚úÖ **Progressive Resizing** - 128px ‚Üí 224px ‚Üí 288px\n",
    "5. ‚úÖ **MixUp** - Data augmentation mixing sample pairs\n",
    "\n",
    "### Additional Optimizations:\n",
    "1. ‚úÖ **Channels Last** - Optimized memory format for GPU\n",
    "\n",
    "### Hardware Configuration:\n",
    "- **GPU:** 1x Apple Silicon (MPS)\n",
    "- **CPU:** 4 data loading workers\n",
    "- **Precision:** 32-bit (MPS compatibility)\n",
    "\n",
    "### Training Schedule:\n",
    "| Epochs | Resolution | Augmentation |\n",
    "|--------|-----------|--------------|\n",
    "| 0-10   | 128x128   | Train (MixUp) |\n",
    "| 10-85  | 224x224   | Train (MixUp) |\n",
    "| 85-90  | 288x288   | Test (FixRes) |\n",
    "\n",
    "**Note:** Uses fixed batch size of 64 throughout all stages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcfb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "import os\n",
    "import antialiased_cnns  # For BlurPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2816cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str, batch_size: int, num_workers: int):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.image_size = 128  # Start with the smallest resolution\n",
    "        self.use_train_augs = True\n",
    "        \n",
    "        # Standard ImageNet normalization\n",
    "        self.normalize = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    def update_resolution(self, image_size: int, use_train_augs: bool, batch_size: int = None):\n",
    "        \"\"\"Called by the callback to update the transforms and optionally batch size.\"\"\"\n",
    "        self.image_size = image_size\n",
    "        self.use_train_augs = use_train_augs\n",
    "        if batch_size is not None:\n",
    "            self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        \"\"\"This is called by the Trainer to get the dataloader.\"\"\"\n",
    "        if self.use_train_augs:\n",
    "            # Standard training augmentations\n",
    "            transform = T.Compose([\n",
    "                T.RandomResizedCrop(self.image_size),\n",
    "                T.RandomHorizontalFlip(),\n",
    "                T.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        else:\n",
    "            # Test-style augmentations for the FixRes phase\n",
    "            transform = T.Compose([\n",
    "                T.Resize(int(self.image_size * 256 / 224)), # Standard practice for validation\n",
    "                T.CenterCrop(self.image_size),\n",
    "                T.ToTensor(),\n",
    "                self.normalize,\n",
    "            ])\n",
    "        \n",
    "        train_dataset = ImageFolder(root=f\"{self.data_dir}/train\", transform=transform)\n",
    "        return DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,  # MPS doesn't support pinned memory\n",
    "            persistent_workers=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # Use the current image size for validation to match training resolution\n",
    "        transform = T.Compose([\n",
    "            T.Resize(int(self.image_size * 256 / 224)),\n",
    "            T.CenterCrop(self.image_size),\n",
    "            T.ToTensor(),\n",
    "            self.normalize,\n",
    "        ])\n",
    "        val_dataset = ImageFolder(root=f\"{self.data_dir}/val\", transform=transform)\n",
    "        return DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            pin_memory=False,  # MPS doesn't support pinned memory\n",
    "            persistent_workers=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1b6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResolutionScheduleCallback(pl.Callback):\n",
    "    def __init__(self, schedule):\n",
    "        super().__init__()\n",
    "        # e.g., {0: (128, True, 512), 10: (224, True, 320), 85: (288, False, 256)}\n",
    "        # Format: {epoch: (resolution, use_train_augs, batch_size)}\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def on_train_epoch_start(self, trainer, pl_module):\n",
    "        if trainer.current_epoch in self.schedule:\n",
    "            config = self.schedule[trainer.current_epoch]\n",
    "            \n",
    "            # Handle both old format (size, use_train_augs) and new format (size, use_train_augs, batch_size)\n",
    "            if len(config) == 2:\n",
    "                size, use_train_augs = config\n",
    "                batch_size = None\n",
    "            else:\n",
    "                size, use_train_augs, batch_size = config\n",
    "            \n",
    "            if batch_size:\n",
    "                print(f\"\\nEpoch {trainer.current_epoch}: Adjusting resolution to {size}x{size}, batch_size to {batch_size} per GPU\")\n",
    "            else:\n",
    "                print(f\"\\nEpoch {trainer.current_epoch}: Adjusting resolution to {size}x{size}\")\n",
    "            \n",
    "            # Update the datamodule's parameters\n",
    "            trainer.datamodule.update_resolution(size, use_train_augs, batch_size)\n",
    "            \n",
    "            # Force recreation of both train and val dataloaders with new transforms\n",
    "            # In PyTorch Lightning 2.x, we need to reload the dataloaders\n",
    "            trainer.fit_loop._data_source.instance = None\n",
    "            trainer.fit_loop.setup_data()\n",
    "            \n",
    "            # Also reset the validation dataloader\n",
    "            if trainer.val_dataloaders is not None:\n",
    "                trainer._evaluation_loop._data_source.instance = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0c3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetLitModule(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=0.1, momentum=0.9, weight_decay=1e-4, mixup_alpha=0.2):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        ## Technique: BlurPool (antialiased downsampling)\n",
    "        self.model = antialiased_cnns.resnet50(pretrained=False, filter_size=4)\n",
    "        \n",
    "        ## Technique: Label Smoothing\n",
    "        self.loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "        \n",
    "        # Initialize accuracy metric\n",
    "        self.val_accuracy = torchmetrics.Accuracy(task='multiclass', num_classes=1000)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        \n",
    "        ## Technique: MixUp - Data augmentation that mixes pairs of samples\n",
    "        lam = torch.distributions.beta.Beta(self.hparams.mixup_alpha, self.hparams.mixup_alpha).sample().to(self.device)\n",
    "        shuffled_indices = torch.randperm(images.size(0))\n",
    "        mixed_images = lam * images + (1 - lam) * images[shuffled_indices]\n",
    "        labels_a, labels_b = labels, labels[shuffled_indices]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = self(mixed_images)\n",
    "        loss = lam * self.loss_fn(outputs, labels_a) + (1 - lam) * self.loss_fn(outputs, labels_b)\n",
    "        \n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, labels = batch\n",
    "        outputs = self(images)\n",
    "        loss = self.loss_fn(outputs, labels)\n",
    "        \n",
    "        # Update accuracy metric\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        self.val_accuracy(preds, labels)\n",
    "        \n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True)\n",
    "        self.log(\"val_acc\", self.val_accuracy, prog_bar=True, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=self.hparams.momentum,\n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf944ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/yash/Documents/ERA/mini-capstone/init/.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "\n",
      "  | Name         | Type               | Params | Mode \n",
      "------------------------------------------------------------\n",
      "0 | model        | ResNet             | 25.6 M | train\n",
      "1 | loss_fn      | CrossEntropyLoss   | 0      | train\n",
      "2 | val_accuracy | MulticlassAccuracy | 0      | train\n",
      "------------------------------------------------------------\n",
      "25.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 M    Total params\n",
      "102.228   Total estimated model params size (MB)\n",
      "171       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: |          | 0/? [00:00<?, ?it/s]                                \n",
      "Epoch 0: Adjusting resolution to 128x128, batch_size to 128 per GPU\n",
      "Epoch 0:   9%|‚ñâ         | 25/272 [00:26<04:22,  0.94it/s, v_num=7, train_loss_step=7.980]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Local ImageNet-Mini path\n",
    "    IMAGENET_PATH = 'imagenet-mini'  # Should contain 'train/' and 'val/' subfolders\n",
    "\n",
    "    # Instantiate the DataModule\n",
    "    # MacBook M4 Pro - optimized batch size for single MPS device\n",
    "    datamodule = ImageNetDataModule(\n",
    "        data_dir=IMAGENET_PATH,\n",
    "        batch_size=64,   # Fixed batch size throughout training\n",
    "        num_workers=4    # Optimized for MacBook M4 Pro\n",
    "    )\n",
    "    \n",
    "    # Instantiate the Model\n",
    "    model = ImageNetLitModule()\n",
    "\n",
    "    ## Technique: Channels Last (optimized memory format for GPU)\n",
    "    model = model.to(memory_format=torch.channels_last)\n",
    "    \n",
    "    ## Technique: Progressive Resizing + FixRes\n",
    "    # Dynamically adjust resolution and augmentation strategy\n",
    "    # Format: {epoch: (resolution, use_train_augs)}\n",
    "    res_schedule = {\n",
    "        0: (128, True),    # Small images, train augmentations\n",
    "        10: (224, True),   # Medium images, train augmentations\n",
    "        85: (288, False)   # Large images, test augmentations (FixRes)\n",
    "    }\n",
    "    schedule_callback = ResolutionScheduleCallback(schedule=res_schedule)\n",
    "\n",
    "    # Configure the Trainer for MacBook M4 Pro (MPS)\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=90,\n",
    "        accelerator='mps',           # Apple Silicon MPS acceleration\n",
    "        devices=1,                   # Single MPS device\n",
    "        precision='32',              # 32-bit precision for MPS compatibility\n",
    "        callbacks=[schedule_callback],\n",
    "        log_every_n_steps=50,\n",
    "    )\n",
    "\n",
    "    # Start training!\n",
    "    trainer.fit(model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00350d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional analysis or experimentation can go here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
